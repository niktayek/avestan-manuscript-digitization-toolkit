{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2f9e28b",
      "metadata": {
        "id": "d2f9e28b"
      },
      "source": [
        "\n",
        "# Kraken + ALTO OCR ‚Äî Colab (Per‚ÄëManuscript Projects, Best‚ÄëModel Saver)\n",
        "\n",
        "This notebook keeps **everything per manuscript** inside a dedicated Drive folder and trains **recognition** models on ALTO data.\n",
        "\n",
        "**What you get**\n",
        "- One-time **Project ID** ‚Üí creates `MyDrive/kraken_projects/<PROJECT_ID>/`\n",
        "- Upload ALTO **ZIP** ‚Üí auto-extracts to `data/`\n",
        "- ALTO-first pairing ‚Üí writes `train.txt` and `val.txt` to `lists/`\n",
        "- **Controls** for: attempt number, learning rate, base model (previous/manual/auto-other)\n",
        "- Training uses **`-t`/`-e`** (Kraken 3) and after training **copies the best checkpoint** to\n",
        "  `models/rec/attempt_XX.mlmodel`\n",
        "- CPU speed tweaks, lean Drive usage (small pip cache)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d85603cc",
      "metadata": {
        "id": "d85603cc"
      },
      "source": [
        "## 1) Connect Google Drive\n",
        "This connects your Google Drive so the notebook can read/write your project files.\n",
        "You‚Äôll be asked to authorize once (click the link, choose your account, allow access).\n",
        "\n",
        "Outcome: Drive mounted at /content/drive.\n",
        "\n",
        "Tip: If you see a warning about ‚Äúalready mounted,‚Äù it‚Äôs fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25c6397",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25c6397",
        "outputId": "5a869952-401c-4468-cb9b-8ce0fd049100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Drive mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive  # type: ignore\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive mounted at /content/drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55530f22",
      "metadata": {
        "id": "55530f22"
      },
      "source": [
        "\n",
        "## 2) Project Settings (edit only `PROJECT_ID`)\n",
        "\n",
        "- `PROJECT_ID`: your manuscript ID (e.g., `0093`).  \n",
        "This cell creates the project folder tree on Drive:\n",
        "```\n",
        "MyDrive/kraken_projects/<PROJECT_ID>/\n",
        "‚îú‚îÄ‚îÄ data/          # upload & extracted dataset (ALTO XML + images)\n",
        "‚îú‚îÄ‚îÄ lists/         # train.txt, val.txt\n",
        "‚îî‚îÄ‚îÄ models/\n",
        "    ‚îî‚îÄ‚îÄ rec/       # attempt_01.mlmodel, attempt_02.mlmodel, ...\n",
        "```\n",
        "\n",
        "What to edit: only the PROJECT_ID (your manuscript number, e.g., 0093).\n",
        "Outcome: Project folders created; paths printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7c729d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e7c729d",
        "outputId": "3e2e383c-8b8f-441a-979a-f7c64ccbc9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Project folders ready\n",
            "PROJECT_DIR: /content/drive/MyDrive/kraken_projects/0093\n",
            "DATA_DIR: /content/drive/MyDrive/kraken_projects/0093/data\n",
            "LISTS_DIR: /content/drive/MyDrive/kraken_projects/0093/lists\n",
            "REC_MODELS: /content/drive/MyDrive/kraken_projects/0093/models/rec\n",
            "CPU_THREADS: 2 | CORES: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#@title üîß Project Settings\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "PROJECT_ID = \"0093\"  #@param {type:\"string\"}\n",
        "\n",
        "ROOT_IN_DRIVE = \"/content/drive/MyDrive\"\n",
        "PROJECTS_ROOT = f\"{ROOT_IN_DRIVE}/kraken_projects\"\n",
        "PROJECT_DIR   = f\"{PROJECTS_ROOT}/{PROJECT_ID}\"\n",
        "DATA_DIR      = f\"{PROJECT_DIR}/data\"\n",
        "LISTS_DIR     = f\"{PROJECT_DIR}/lists\"\n",
        "REC_MODELS    = f\"{PROJECT_DIR}/models/rec\"\n",
        "PIP_CACHE_DIR = f\"{ROOT_IN_DRIVE}/.pip-cache\"   # small cache only\n",
        "\n",
        "# Create the full project tree\n",
        "for p in [PROJECTS_ROOT, PROJECT_DIR, DATA_DIR, LISTS_DIR, REC_MODELS, PIP_CACHE_DIR]:\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_LIST = f\"{LISTS_DIR}/train.txt\"\n",
        "VAL_LIST   = f\"{LISTS_DIR}/val.txt\"\n",
        "\n",
        "# CPU basics\n",
        "CORES = os.cpu_count() or 2\n",
        "CPU_THREADS = max(2, CORES - 1)\n",
        "DEVICE = \"cpu\"   # set \"cuda\" if you enable a T4 GPU in Colab\n",
        "\n",
        "print(\"‚úÖ Project folders ready\")\n",
        "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"LISTS_DIR:\", LISTS_DIR)\n",
        "print(\"REC_MODELS:\", REC_MODELS)\n",
        "print(\"CPU_THREADS:\", CPU_THREADS, \"| CORES:\", CORES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a2ad23",
      "metadata": {
        "id": "76a2ad23"
      },
      "source": [
        "## 3) CPU Speed Boost (threads & math libs)\n",
        "\n",
        "Sets thread/env variables so image and math libraries don‚Äôt oversubscribe CPU cores.\n",
        "Nothing to edit.\n",
        "Outcome: Printed thread settings (this can shave seconds on CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf90e4c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf90e4c7",
        "outputId": "9f8d919c-4af3-4de4-e16b-ac2d9ea47449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OMP_NUM_THREADS = 2\n",
            "MKL_NUM_THREADS = 2\n",
            "OPENBLAS_NUM_THREADS = 2\n",
            "NUMEXPR_NUM_THREADS = 2\n",
            "KMP_BLOCKTIME = 1\n",
            "KMP_AFFINITY = granularity=fine,compact,1,0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"OMP_NUM_THREADS\"] = str(CPU_THREADS)\n",
        "os.environ[\"MKL_NUM_THREADS\"] = str(CPU_THREADS)\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(CPU_THREADS)\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(CPU_THREADS)\n",
        "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
        "os.environ[\"KMP_SETTINGS\"] = \"0\"\n",
        "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
        "\n",
        "for k in [\"OMP_NUM_THREADS\",\"MKL_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"KMP_BLOCKTIME\",\"KMP_AFFINITY\"]:\n",
        "    print(k, \"=\", os.environ.get(k))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55405fe0",
      "metadata": {
        "id": "55405fe0"
      },
      "source": [
        "## 4) Install Kraken (lean) + Pillow‚ÄëSIMD (faster image I/O)\n",
        "Installs Kraken OCR and a faster Pillow build for image I/O.\n",
        "The cell only installs if Kraken isn‚Äôt present.\n",
        "Nothing to edit.\n",
        "Outcome: ‚ÄúKraken available/installed‚Äù and ‚ÄúPillow-SIMD installed‚Äù."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ea2704",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ea2704",
        "outputId": "fda546b1-2c9a-4d05-a034-65151aceab67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Installing Kraken (lean) ...\n",
            "‚úÖ Installed Kraken (version: unknown)\n",
            "‚è≥ Switching to Pillow-SIMD for faster image ops...\n",
            "‚úÖ Pillow-SIMD installed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install Kraken (lean) + Pillow-SIMD (faster image IO)\n",
        "\n",
        "import os, subprocess, sys\n",
        "\n",
        "# Lean pip settings\n",
        "os.environ[\"PIP_CACHE_DIR\"] = PIP_CACHE_DIR\n",
        "os.environ[\"PIP_DISABLE_PIP_VERSION_CHECK\"] = \"1\"\n",
        "os.environ[\"PIP_NO_INPUT\"] = \"1\"\n",
        "\n",
        "def is_importable(pkg: str) -> bool:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Install kraken only if missing\n",
        "if is_importable(\"kraken\"):\n",
        "    import kraken\n",
        "    print(f\"‚úÖ Kraken available (version: {getattr(kraken, '__version__', 'unknown')})\")\n",
        "else:\n",
        "    print(\"‚è≥ Installing Kraken (lean) ...\")\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", \"--upgrade\", \"pip\"], check=True)\n",
        "    subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n",
        "        \"--prefer-binary\",\n",
        "        \"--upgrade-strategy\", \"only-if-needed\",\n",
        "        \"kraken[cairo]\"\n",
        "    ], check=True)\n",
        "    import kraken, importlib\n",
        "    importlib.reload(kraken)\n",
        "    print(f\"‚úÖ Installed Kraken (version: {getattr(kraken, '__version__', 'unknown')})\")\n",
        "\n",
        "# Switch to Pillow-SIMD for faster image decoding/resizing\n",
        "print(\"‚è≥ Switching to Pillow-SIMD for faster image ops...\")\n",
        "# Don't fail the whole cell if Pillow isn't present yet\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"uninstall\", \"-y\", \"pillow\"], check=False)\n",
        "subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n",
        "    \"--prefer-binary\",\n",
        "    \"--upgrade-strategy\", \"only-if-needed\",\n",
        "    \"pillow-simd\"\n",
        "], check=True)\n",
        "print(\"‚úÖ Pillow-SIMD installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d00b6c2",
      "metadata": {
        "id": "6d00b6c2"
      },
      "source": [
        "## 5) Upload your ALTO dataset (ZIP ‚Üí Drive project folder)\n",
        "Upload your ZIP containing images and ALTO XML files.\n",
        "The ZIP is extracted into your project‚Äôs data/ folder on Drive.\n",
        "\n",
        "What to do: Click the upload button and pick your ZIP.\n",
        "Outcome: File listing of a few extracted files.\n",
        "\n",
        "Your ZIP should contain paired page images (e.g., .tif/.jpg) and ALTO XMLs. The XMLs must include <fileName> elements referring to the page image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e23c5b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "6e23c5b0",
        "outputId": "b809fa78-15bb-4cab-a0be-2321a18bcded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Please select your ZIP (ALTO XML + images)‚Ä¶\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-412ac874-ea0f-40d4-b53d-c62cd922398b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-412ac874-ea0f-40d4-b53d-c62cd922398b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving export_doc6047_4000_mirrored_alto_202511211531.zip to export_doc6047_4000_mirrored_alto_202511211531.zip\n",
            "‚úÖ Uploaded: export_doc6047_4000_mirrored_alto_202511211531.zip\n",
            "üìÇ Extracting into: /content/drive/MyDrive/kraken_projects/0093/data\n",
            "‚úÖ Extraction done. Showing a few files:\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_073v_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_074r_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_072v_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_073r_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_074v_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_075r_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/Ave976_075v_mir.xml\n",
            "/content/drive/MyDrive/kraken_projects/0093/data/METS.xml\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "# Ensure Drive is mounted\n",
        "if not Path(\"/content/drive/MyDrive\").exists():\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "# Ensure PROJECT_ID exists (fallback is ok; better if set earlier)\n",
        "if 'PROJECT_ID' not in globals():\n",
        "    PROJECT_ID = \"0093\"  # fallback; user can change upstream\n",
        "\n",
        "BASE_DIR = Path(f\"/content/drive/MyDrive/kraken_projects/{PROJECT_ID}\")\n",
        "DATA_DIR = BASE_DIR / \"data\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"üì¶ Please select your ZIP (ALTO XML + images)‚Ä¶\")\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"‚ùå No file uploaded.\")\n",
        "\n",
        "zip_name = next(iter(uploaded.keys()))\n",
        "zip_path = Path(f\"/content/{zip_name}\")\n",
        "\n",
        "print(f\"‚úÖ Uploaded: {zip_name}\")\n",
        "print(f\"üìÇ Extracting into: {DATA_DIR}\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(DATA_DIR)\n",
        "\n",
        "print(\"‚úÖ Extraction done. Showing a few files:\")\n",
        "!find \"{DATA_DIR}\" -maxdepth 2 -type f | head -n 20\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9127fd6c",
      "metadata": {
        "id": "9127fd6c"
      },
      "source": [
        "## 6) Build train/val lists from ALTO (in project folder)\n",
        "Scans data/ for ALTO XMLs and writes:\n",
        "\t‚Ä¢\tlists/train.txt (90%)\n",
        "\t‚Ä¢\tlists/val.txt (10%)\n",
        "\n",
        "Important: For -f alto, lists must contain only XML file paths, one per line‚Äîno image paths.\n",
        "Outcome: Counts printed + a short sample of each list.\n",
        "\n",
        "If it says ‚ÄúNot enough ALTO XMLs‚Äù or shows missing files, check your ZIP structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bb1878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0bb1878",
        "outputId": "20855fb0-63e2-42be-d9af-6006d4e0f753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 ALTO XML files.\n",
            "‚úÖ Wrote lists:\n",
            "  - /content/drive/MyDrive/kraken_projects/0093/lists/train.txt: 6\n",
            "  - /content/drive/MyDrive/kraken_projects/0093/lists/val.txt:   1\n",
            "\n",
            "Sample from /content/drive/MyDrive/kraken_projects/0093/lists/train.txt:\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_073r_mir.xml\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_074r_mir.xml\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_074v_mir.xml\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_073v_mir.xml\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_075v_mir.xml\n",
            "\n",
            "Sample from /content/drive/MyDrive/kraken_projects/0093/lists/val.txt:\n",
            "   /content/drive/MyDrive/kraken_projects/0093/data/Ave976_075r_mir.xml\n"
          ]
        }
      ],
      "source": [
        "# Build train/val lists for ALTO: ONE XML PATH PER LINE (no image paths)\n",
        "\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "import random, os\n",
        "\n",
        "random.seed(42)  # reproducible split\n",
        "\n",
        "DATA_DIR = DATA_DIR  # already defined\n",
        "LISTS_DIR = LISTS_DIR\n",
        "TRAIN_LIST = TRAIN_LIST\n",
        "VAL_LIST = VAL_LIST\n",
        "\n",
        "def is_alto_xml(p: Path) -> bool:\n",
        "    if p.suffix.lower() != \".xml\" or not p.is_file():\n",
        "        return False\n",
        "    try:\n",
        "        # quick scan first\n",
        "        with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "            head = fh.read(4096)\n",
        "            if \"<alto\" not in head:\n",
        "                return False\n",
        "        # light parse to be safe\n",
        "        ET.parse(p)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Collect all ALTO XMLs under DATA_DIR\n",
        "xmls = [p for p in Path(DATA_DIR).rglob(\"*.xml\") if is_alto_xml(p)]\n",
        "xmls = sorted({x.resolve() for x in xmls})  # dedup + sort\n",
        "\n",
        "n = len(xmls)\n",
        "print(f\"Found {n} ALTO XML files.\")\n",
        "\n",
        "if n < 2:\n",
        "    raise SystemExit(f\"‚ùå Not enough ALTO XMLs in {DATA_DIR}. Found {n}.\")\n",
        "\n",
        "# 90/10 split, shuffled\n",
        "random.shuffle(xmls)\n",
        "cut = max(1, int(n * 0.9))\n",
        "train_xmls, val_xmls = xmls[:cut], xmls[cut:]\n",
        "\n",
        "Path(LISTS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "with open(TRAIN_LIST, \"w\", encoding=\"utf-8\") as f:\n",
        "    for x in train_xmls:\n",
        "        f.write(str(x) + \"\\n\")\n",
        "with open(VAL_LIST, \"w\", encoding=\"utf-8\") as f:\n",
        "    for x in val_xmls:\n",
        "        f.write(str(x) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Wrote lists:\")\n",
        "print(f\"  - {TRAIN_LIST}: {len(train_xmls)}\")\n",
        "print(f\"  - {VAL_LIST}:   {len(val_xmls)}\")\n",
        "\n",
        "# quick sanity check\n",
        "def show_sample(fp, k=5):\n",
        "    print(f\"\\nSample from {fp}:\")\n",
        "    with open(fp, encoding=\"utf-8\") as fh:\n",
        "        for i, line in enumerate(fh):\n",
        "            if i >= k: break\n",
        "            print(\"  \", line.strip())\n",
        "show_sample(TRAIN_LIST)\n",
        "show_sample(VAL_LIST)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76765693",
      "metadata": {
        "id": "76765693"
      },
      "source": [
        "\n",
        "## 7) Training Controls\n",
        "\n",
        "Choose your attempt, learning rate, and base model source:\n",
        "\t‚Ä¢\tBASE_SOURCE = choose ‚Üí type the attempt number to load from this manuscript (e.g., 13).\n",
        "\t‚Ä¢\tBASE_SOURCE = upload ‚Üí you‚Äôll upload a .mlmodel as base.\n",
        "\n",
        "Inputs to fill:\n",
        "\t‚Ä¢\tATTEMPT_ID, LEARNING_RATE, BASE_SOURCE, and if needed BASE_ATTEMPT_TO_LOAD.\n",
        "Outcome: The chosen values are printed for confirmation.\n",
        "\n",
        "Tip: On CPU, try BATCH_SIZE = 32 (bump to 48/64 if it fits).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b07f678",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b07f678",
        "outputId": "9ba32404-c68f-40c8-c9f8-e4e204467997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATTEMPT_ID: 1\n",
            "LEARNING_RATE: 0.0001\n",
            "BASE_SOURCE: upload\n",
            "BASE_ATTEMPT_TO_LOAD: None\n"
          ]
        }
      ],
      "source": [
        "#@title üß≠ Training Controls (choose or upload a base; no paths)\n",
        "ATTEMPT_ID = 1            #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.0001     #@param {type:\"number\"}  # set to 0 to use Kraken default\n",
        "BASE_SOURCE = \"upload\"     #@param [\"choose\", \"upload\"]\n",
        "BASE_ATTEMPT_TO_LOAD = None  #@param {type:\"number\"}  # attempt to load if BASE_SOURCE=\"choose\"\n",
        "\n",
        "# Early stopping\n",
        "EARLY_STOP = \"early\"       #@param [\"early\", \"dumb\"]\n",
        "LAG_EPOCHS = 6             #@param {type:\"number\"}\n",
        "MIN_DELTA = 0.0001         #@param {type:\"number\"}\n",
        "\n",
        "# CPU perf\n",
        "import os\n",
        "BATCH_SIZE = min(32, max(8, (os.cpu_count() or 2) * 2))\n",
        "\n",
        "# Normalize LR (0 => None to use Kraken default)\n",
        "LEARNING_RATE = None if (LEARNING_RATE == 0) else LEARNING_RATE\n",
        "\n",
        "print(\"ATTEMPT_ID:\", ATTEMPT_ID)\n",
        "print(\"LEARNING_RATE:\", LEARNING_RATE)\n",
        "print(\"BASE_SOURCE:\", BASE_SOURCE)\n",
        "print(\"BASE_ATTEMPT_TO_LOAD:\", BASE_ATTEMPT_TO_LOAD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MPnrHdbOZW6l",
      "metadata": {
        "id": "MPnrHdbOZW6l"
      },
      "source": [
        "## üîó Resolve Base Model (STRICT: choose attempt or upload; no fallback)\n",
        "Picks the base .mlmodel automatically without asking for paths:\n",
        "\t‚Ä¢\tIf choose, it loads attempt_XX.mlmodel from this manuscript‚Äôs models/rec/.\n",
        "\t‚Ä¢\tIf upload, it opens a file picker and saves your upload to the right place.\n",
        "\n",
        "Outcome: Prints the exact BASE_MODEL path. If a chosen attempt isn‚Äôt found, it lists available attempts.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rjKFQxjOYiX0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "rjKFQxjOYiX0",
        "outputId": "83a63368-2571-4491-9930-3cef899a7ed2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì§ Upload a .mlmodel file to use as base‚Ä¶\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2592566a-7c23-4d9b-9ea1-81d10fa06f41\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2592566a-7c23-4d9b-9ea1-81d10fa06f41\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 0089_rec_01.mlmodel to 0089_rec_01.mlmodel\n",
            "‚úÖ Uploaded base to: /content/drive/MyDrive/kraken_projects/0093/models/rec/0089_rec_01.mlmodel\n",
            "\n",
            "‚úÖ BASE_MODEL resolved as: /content/drive/MyDrive/kraken_projects/0093/models/rec/0089_rec_01.mlmodel\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files  # type: ignore\n",
        "import shutil\n",
        "\n",
        "# Clear any stale value\n",
        "if 'BASE_MODEL' in globals():\n",
        "    del BASE_MODEL\n",
        "\n",
        "def list_available_attempts(rec_dir: str):\n",
        "    recp = Path(rec_dir)\n",
        "    found = sorted(p for p in recp.glob(\"attempt_*.mlmodel\"))\n",
        "    if not found:\n",
        "        print(\"‚ÑπÔ∏è No attempts found yet in\", recp)\n",
        "        return\n",
        "    print(\"Available attempts in this manuscript:\")\n",
        "    for p in found:\n",
        "        print(\"  ‚Ä¢\", p.name)\n",
        "\n",
        "def upload_model_to_drive(dest_dir: str) -> str:\n",
        "    print(\"üì§ Upload a .mlmodel file to use as base‚Ä¶\")\n",
        "    up = files.upload()\n",
        "    if not up:\n",
        "        raise SystemExit(\"‚ùå No file uploaded.\")\n",
        "    fname = next(iter(up.keys()))\n",
        "    src = f\"/content/{fname}\"\n",
        "    dest = str(Path(dest_dir) / Path(fname).name)\n",
        "    shutil.move(src, dest)\n",
        "    print(f\"‚úÖ Uploaded base to: {dest}\")\n",
        "    return dest\n",
        "\n",
        "BASE_MODEL = None\n",
        "\n",
        "if BASE_SOURCE == \"choose\":\n",
        "    chosen = Path(REC_MODELS) / f\"attempt_{int(BASE_ATTEMPT_TO_LOAD):02d}.mlmodel\"\n",
        "    if chosen.exists():\n",
        "        BASE_MODEL = str(chosen.resolve())\n",
        "        print(\"Base = chosen attempt:\", BASE_MODEL)\n",
        "    else:\n",
        "        print(f\"‚ùå attempt_{int(BASE_ATTEMPT_TO_LOAD):02d}.mlmodel not found in {REC_MODELS}.\")\n",
        "        list_available_attempts(REC_MODELS)\n",
        "        raise SystemExit(\"Change BASE_ATTEMPT_TO_LOAD or switch to BASE_SOURCE='upload'.\")\n",
        "\n",
        "elif BASE_SOURCE == \"upload\":\n",
        "    BASE_MODEL = upload_model_to_drive(REC_MODELS)\n",
        "\n",
        "print(\"\\n‚úÖ BASE_MODEL resolved as:\", BASE_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZNRXaGczgaZI",
      "metadata": {
        "id": "ZNRXaGczgaZI"
      },
      "source": [
        "## Stage ALTO dataset locally (XMLs + images; rewrite <fileName>)\n",
        "Copies each XML in the lists and its linked image to local disk (/content/alto_staged/data/), then rewrites the XML‚Äôs <fileName> to the local image name. This removes Google Drive latency.\n",
        "\n",
        "Nothing to edit.\n",
        "Outcome: Creates:\n",
        "\t‚Ä¢\tLOCAL_TRAIN_LIST and LOCAL_VAL_LIST with updated local XML paths\n",
        "\t‚Ä¢\tA count of staged entries\n",
        "\n",
        "On CPU, this is the biggest speed boost. On GPU, it‚Äôs optional (still helps a bit).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h5frN8T4gWNd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "h5frN8T4gWNd",
        "outputId": "a96e46b7-f9f3-412b-9c9d-40303dfb0ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Staged 0 entries ‚Üí /content/alto_staged/train_local.txt\n",
            "‚ö†Ô∏è Skipped 6 items (showing up to 5):\n",
            "  - Image not found for: /content/drive/MyDrive/kraken_projects/0093/data/Ave976_073r_mir.xml (<fileName>=Ave976_073r_mir.jpg)\n",
            "  - Image not found for: /content/drive/MyDrive/kraken_projects/0093/data/Ave976_074r_mir.xml (<fileName>=Ave976_074r_mir.jpg)\n",
            "  - Image not found for: /content/drive/MyDrive/kraken_projects/0093/data/Ave976_074v_mir.xml (<fileName>=Ave976_074v_mir.jpg)\n",
            "  - Image not found for: /content/drive/MyDrive/kraken_projects/0093/data/Ave976_073v_mir.xml (<fileName>=Ave976_073v_mir.jpg)\n",
            "  - Image not found for: /content/drive/MyDrive/kraken_projects/0093/data/Ave976_075v_mir.xml (<fileName>=Ave976_075v_mir.jpg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Staging produced 0 entries.\nThis usually means your ALTO XML references images that are not available next to the XML\nor inside a common subfolder (images/img/pages/page), OR your export did not include images.\nFix: ensure the export includes the page images, or place them into one of those folders.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3687884308.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# Run staging for train/val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mn_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stage_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOCAL_TRAIN_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_OUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0mn_va\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stage_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAL_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOCAL_VAL_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_OUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3687884308.py\u001b[0m in \u001b[0;36m_stage_list\u001b[0;34m(list_path, out_list_path, data_out)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# Fail early if empty so users aren't confused later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstaged\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;34m\"Staging produced 0 entries.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;34m\"This usually means your ALTO XML references images that are not available next to the XML\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Staging produced 0 entries.\nThis usually means your ALTO XML references images that are not available next to the XML\nor inside a common subfolder (images/img/pages/page), OR your export did not include images.\nFix: ensure the export includes the page images, or place them into one of those folders."
          ]
        }
      ],
      "source": [
        "#  Run staging for train/val (robust ALTO image resolution)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "# Expected inputs from earlier cells:\n",
        "#   TRAIN_LIST, VAL_LIST\n",
        "# Outputs used by training:\n",
        "#   LOCAL_TRAIN_LIST, LOCAL_VAL_LIST, _USE_LOCAL\n",
        "#\n",
        "# If your notebook uses different variable names, adjust them here:\n",
        "if 'TRAIN_LIST' not in globals():\n",
        "    TRAIN_LIST = \"/content/drive/MyDrive/kraken_projects/0093/lists/train.txt\"\n",
        "if 'VAL_LIST' not in globals():\n",
        "    VAL_LIST = \"/content/drive/MyDrive/kraken_projects/0093/lists/val.txt\"\n",
        "\n",
        "LOCAL_ROOT = Path(\"/content/alto_staged\")\n",
        "DATA_OUT = LOCAL_ROOT / \"data\"\n",
        "LOCAL_TRAIN_LIST = LOCAL_ROOT / \"train_local.txt\"\n",
        "LOCAL_VAL_LIST = LOCAL_ROOT / \"val_local.txt\"\n",
        "\n",
        "LOCAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "DATA_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Common export layouts (kept simple so users don't have to think)\n",
        "IMAGE_SUBFOLDERS = [\"\", \"images\", \"img\", \"pages\", \"page\"]\n",
        "IMAGE_EXTS = [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]\n",
        "\n",
        "def _read_alto_filename(xml_text: str) -> str | None:\n",
        "    \"\"\"Extracts <fileName>...</fileName> from ALTO XML.\"\"\"\n",
        "    m = re.search(r\"<fileName>\\s*([^<]+)\\s*</fileName>\", xml_text)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def _find_image_for_alto(xml_path: Path, file_name: str) -> Path | None:\n",
        "    \"\"\"Finds the image file referenced by ALTO, searching common folders and extension/case variants.\"\"\"\n",
        "    # 1) Try exact file name in common subfolders\n",
        "    for sub in IMAGE_SUBFOLDERS:\n",
        "        candidate = xml_path.parent / sub / file_name\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "\n",
        "    # 2) Try stem + extension variants (and upper-case extensions)\n",
        "    stem = Path(file_name).stem\n",
        "    for sub in IMAGE_SUBFOLDERS:\n",
        "        base = xml_path.parent / sub\n",
        "        for ext in IMAGE_EXTS:\n",
        "            for variant in (ext, ext.upper()):\n",
        "                candidate = base / (stem + variant)\n",
        "                if candidate.exists():\n",
        "                    return candidate\n",
        "\n",
        "    return None\n",
        "\n",
        "def _stage_list(list_path: str, out_list_path: Path, data_out: Path) -> int:\n",
        "    \"\"\"\n",
        "    Stages ALTO XMLs + referenced images into `data_out` and writes a local list file.\n",
        "    Returns number of staged entries.\n",
        "    \"\"\"\n",
        "    list_path = Path(list_path)\n",
        "    out_list_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    staged = 0\n",
        "    missing = []\n",
        "\n",
        "    with list_path.open(\"r\", encoding=\"utf-8\") as f_in, out_list_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
        "        for raw in f_in:\n",
        "            raw = raw.strip()\n",
        "            if not raw:\n",
        "                continue\n",
        "\n",
        "            src_xml = Path(raw)\n",
        "            if not src_xml.exists():\n",
        "                missing.append(f\"Missing XML: {src_xml}\")\n",
        "                continue\n",
        "\n",
        "            xml_text = src_xml.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "            file_name = _read_alto_filename(xml_text)\n",
        "\n",
        "            if not file_name:\n",
        "                missing.append(f\"No <fileName> in: {src_xml}\")\n",
        "                continue\n",
        "\n",
        "            img_path = _find_image_for_alto(src_xml, file_name)\n",
        "            if img_path is None:\n",
        "                missing.append(f\"Image not found for: {src_xml} (<fileName>={file_name})\")\n",
        "                continue\n",
        "\n",
        "            # Copy XML into local staging\n",
        "            dst_xml = data_out / src_xml.name\n",
        "            if not dst_xml.exists():\n",
        "                shutil.copy2(src_xml, dst_xml)\n",
        "\n",
        "            # Copy image into local staging under the exact <fileName> expected by ALTO\n",
        "            dst_img = data_out / file_name\n",
        "            if not dst_img.exists():\n",
        "                shutil.copy2(img_path, dst_img)\n",
        "\n",
        "            # Write staged XML path into local list (Kraken reads image via ALTO reference)\n",
        "            f_out.write(str(dst_xml) + \"\\n\")\n",
        "            staged += 1\n",
        "\n",
        "    print(f\"‚úÖ Staged {staged} entries ‚Üí {out_list_path}\")\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è Skipped {len(missing)} items (showing up to 5):\")\n",
        "        for m in missing[:5]:\n",
        "            print(\"  -\", m)\n",
        "\n",
        "    # Fail early if empty so users aren't confused later\n",
        "    if staged == 0:\n",
        "        raise RuntimeError(\n",
        "            \"Staging produced 0 entries.\\n\"\n",
        "            \"This usually means your ALTO XML references images that are not available next to the XML\\n\"\n",
        "            \"or inside a common subfolder (images/img/pages/page), OR your export did not include images.\\n\"\n",
        "            \"Fix: ensure the export includes the page images, or place them into one of those folders.\"\n",
        "        )\n",
        "\n",
        "    return staged\n",
        "\n",
        "# Run staging for train/val\n",
        "n_tr = _stage_list(TRAIN_LIST, LOCAL_TRAIN_LIST, DATA_OUT)\n",
        "n_va = _stage_list(VAL_LIST, LOCAL_VAL_LIST, DATA_OUT)\n",
        "\n",
        "print(\"\\nLocal staged dataset:\")\n",
        "print(\"  Root:\", LOCAL_ROOT)\n",
        "print(\"  Data dir:\", DATA_OUT)\n",
        "print(\"  TRAIN (local):\", LOCAL_TRAIN_LIST, \"| entries:\", n_tr)\n",
        "print(\"  VAL   (local):\", LOCAL_VAL_LIST,   \"| entries:\", n_va)\n",
        "\n",
        "# Tell training cell to use staged lists\n",
        "_USE_LOCAL = True\n",
        "print(\"\\nüëâ In your training cell set: _USE_LOCAL = True (or keep it enabled if already true).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e17be092",
      "metadata": {
        "id": "e17be092"
      },
      "source": [
        "[link text](https://)\n",
        "## 8) Train recognition (best checkpoint ‚Üí `attempt_XX.mlmodel`)\n",
        "\n",
        "Trains with live logs (you‚Äôll see epochs and metrics as they happen).\n",
        "The best checkpoint is saved as:\n",
        "MyDrive/kraken_projects/<PROJECT_ID>/models/rec/attempt_XX.mlmodel\n",
        "\n",
        "What to check before running:\n",
        "\t‚Ä¢\tBASE_MODEL is printed correctly (from ‚ÄúResolve Base Model‚Äù).\n",
        "\t‚Ä¢\t_USE_LOCAL = True if you ran ‚ÄúStage ALTO‚Äù (recommended on CPU).\n",
        "\n",
        "Outcome: Live training logs; at the end you‚Äôll see ‚ÄúSaved best checkpoint as ‚Ä¶‚Äù.\n",
        "\n",
        "Slow on CPU? Use staging, increase BATCH_SIZE, validate less often (set -F 2), or switch Colab runtime to GPU (T4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9e707d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8b9e707d",
        "outputId": "6245222b-8bd3-420c-b072-7a6509be2576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Project ID:      0093\n",
            "üìÇ BASE_DIR:        /content/drive/MyDrive/kraken_projects/0093\n",
            "üìÑ Train list:      /content/drive/MyDrive/kraken_projects/0093/lists/train.txt\n",
            "üìÑ Val list:        /content/drive/MyDrive/kraken_projects/0093/lists/val.txt\n",
            "üì¶ Output prefix:   /content/drive/MyDrive/kraken_projects/0093/models/rec/attempt_01\n",
            "üß† Base model:      /content/drive/MyDrive/kraken_projects/0093/models/rec/0088-recognizer-02.mlmodel\n",
            "\n",
            "Running:\n",
            "ketos train -o /content/drive/MyDrive/kraken_projects/0093/models/rec/attempt_01 -f alto -t\n",
            "/content/drive/MyDrive/kraken_projects/0093/lists/train.txt -e /content/drive/MyDrive/kraken_projects/0093/lists/val.txt\n",
            "-q early --lag 6 --min-delta 0.0001 -B 8 -F 1 -i\n",
            "/content/drive/MyDrive/kraken_projects/0093/models/rec/0088-recognizer-02.mlmodel --resize add -r 0.0001\n",
            "--------------------------------------------------------------------------------\n",
            "2025-11-29 20:13:01.458420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447181.811092    3795 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447181.913836    3795 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447182.545714    3795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447182.545767    3795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447182.545772    3795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447182.545774    3795 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:13:02.605054: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[11/29/25 20:13:16] WARNING  alphabet mismatch: chars in training   train.py:428\n",
            "                             set only: {'Ãä', 'j', 'k', 'Ãå', '·π£', '≈´',             \n",
            "                             'Ã∞', 'o'} (not included in accuracy                 \n",
            "                             test during training)                              \n",
            "WARNING:kraken.lib.train:alphabet mismatch: chars in training set only: {'Ãä', 'j', 'k', 'Ãå', '·π£', '≈´', 'Ã∞', 'o'} (not included in accuracy test during training)\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
            "[11/29/25 20:13:17] WARNING  Neural network has been trained on     train.py:639\n",
            "                             mode L images, training set contains               \n",
            "                             mode 1 data. Consider setting                      \n",
            "                             `force_binarization`                               \n",
            "WARNING:kraken.lib.train:Neural network has been trained on mode L images, training set contains mode 1 data. Consider setting `force_binarization`\n",
            "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
            "‚îÉ    ‚îÉ Name      ‚îÉ Type         ‚îÉ Params ‚îÉ Mode  ‚îÉ     In sizes ‚îÉ    Out sizes ‚îÉ\n",
            "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
            "‚îÇ 0  ‚îÇ val_cer   ‚îÇ CharErrorRa‚Ä¶ ‚îÇ      0 ‚îÇ train ‚îÇ            ? ‚îÇ            ? ‚îÇ\n",
            "‚îÇ 1  ‚îÇ val_wer   ‚îÇ WordErrorRa‚Ä¶ ‚îÇ      0 ‚îÇ train ‚îÇ            ? ‚îÇ            ? ‚îÇ\n",
            "‚îÇ 2  ‚îÇ net       ‚îÇ MultiParamS‚Ä¶ ‚îÇ  4.0 M ‚îÇ train ‚îÇ [[1, 1, 120, ‚îÇ  [[1, 60, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   400], '?'] ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ 3  ‚îÇ net.C_0   ‚îÇ ActConv2D    ‚îÇ  1.3 K ‚îÇ train ‚îÇ [[1, 1, 120, ‚îÇ     [[1, 32, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   400], '?', ‚îÇ   120, 400], ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ         '?'] ‚îÇ\n",
            "‚îÇ 4  ‚îÇ net.Do_1  ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ     [[1, 32, ‚îÇ     [[1, 32, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   120, 400], ‚îÇ   120, 400], ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    '?', '?'] ‚îÇ         '?'] ‚îÇ\n",
            "‚îÇ 5  ‚îÇ net.Mp_2  ‚îÇ MaxPool      ‚îÇ      0 ‚îÇ train ‚îÇ     [[1, 32, ‚îÇ [[1, 32, 60, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   120, 400], ‚îÇ   200], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    '?', '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 6  ‚îÇ net.C_3   ‚îÇ ActConv2D    ‚îÇ 40.0 K ‚îÇ train ‚îÇ [[1, 32, 60, ‚îÇ [[1, 32, 60, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   200], '?', ‚îÇ   200], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 7  ‚îÇ net.Do_4  ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 32, 60, ‚îÇ [[1, 32, 60, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   200], '?', ‚îÇ   200], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 8  ‚îÇ net.Mp_5  ‚îÇ MaxPool      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 32, 60, ‚îÇ [[1, 32, 30, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   200], '?', ‚îÇ   100], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 9  ‚îÇ net.C_6   ‚îÇ ActConv2D    ‚îÇ 55.4 K ‚îÇ train ‚îÇ [[1, 32, 30, ‚îÇ [[1, 64, 30, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   100], '?', ‚îÇ   100], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 10 ‚îÇ net.Do_7  ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 64, 30, ‚îÇ [[1, 64, 30, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   100], '?', ‚îÇ   100], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 11 ‚îÇ net.Mp_8  ‚îÇ MaxPool      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 64, 30, ‚îÇ [[1, 64, 15, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ   100], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 12 ‚îÇ net.C_9   ‚îÇ ActConv2D    ‚îÇ  110 K ‚îÇ train ‚îÇ [[1, 64, 15, ‚îÇ [[1, 64, 15, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 13 ‚îÇ net.Do_10 ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 64, 15, ‚îÇ [[1, 64, 15, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 14 ‚îÇ net.S_11  ‚îÇ Reshape      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 64, 15, ‚îÇ [[1, 960, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 15 ‚îÇ net.L_12  ‚îÇ TransposedS‚Ä¶ ‚îÇ  1.9 M ‚îÇ train ‚îÇ [[1, 960, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 16 ‚îÇ net.Do_13 ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 17 ‚îÇ net.L_14  ‚îÇ TransposedS‚Ä¶ ‚îÇ  963 K ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 18 ‚îÇ net.Do_15 ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 19 ‚îÇ net.L_16  ‚îÇ TransposedS‚Ä¶ ‚îÇ  963 K ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 20 ‚îÇ net.Do_17 ‚îÇ Dropout      ‚îÇ      0 ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ [[1, 400, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îÇ 21 ‚îÇ net.O_18  ‚îÇ LinSoftmax   ‚îÇ 24.1 K ‚îÇ train ‚îÇ [[1, 400, 1, ‚îÇ  [[1, 60, 1, ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ    50], '?', ‚îÇ    50], '?'] ‚îÇ\n",
            "‚îÇ    ‚îÇ           ‚îÇ              ‚îÇ        ‚îÇ       ‚îÇ         '?'] ‚îÇ              ‚îÇ\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "Trainable params: 4.0 M                                                         \n",
            "Non-trainable params: 0                                                         \n",
            "Total params: 4.0 M                                                             \n",
            "Total estimated model params size (MB): 16                                      \n",
            "Modules in train mode: 40                                                       \n",
            "Modules in eval mode: 0                                                         \n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py:516: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 6. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py:516: You called `self.log('val_accuracy', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py:516: You called `self.log('val_word_accuracy', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/lightning/pytorch/core/module.py:516: You called `self.log('val_metric', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "\n",
            "stage 0/‚àû ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13/13 0:02:05 ‚Ä¢     0.12it/s train_loss_s‚Ä¶ early_stoppi‚Ä¶\n",
            "                              0:00:00                554.672       0/6 0.88721  \n",
            "                                                     val_accuracy:              \n",
            "                                                     0.887                      \n",
            "                                                     val_word_acc‚Ä¶              \n",
            "                                                     0.388                      \n",
            "                                                     train_loss_e‚Ä¶              \n",
            "                                                     708.922                    \n",
            "\n",
            "‚õî Training interrupted by user. Trying to stop ketos‚Ä¶\n",
            "\n",
            "‚ö†Ô∏è No _best.mlmodel found yet (did training reach at least one epoch?)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2023774200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2023774200.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shlex\n",
        "import textwrap\n",
        "\n",
        "# Try to infer PROJECT_ID and BASE_DIR if they are not already defined\n",
        "\n",
        "\n",
        "if 'PROJECT_ID' not in globals():\n",
        "    if 'BASE_MODEL' in globals() and BASE_MODEL:\n",
        "        parts = BASE_MODEL.split('/')\n",
        "        if \"kraken_projects\" in parts:\n",
        "            idx = parts.index(\"kraken_projects\")\n",
        "            if idx + 1 < len(parts):\n",
        "                PROJECT_ID = parts[idx + 1]\n",
        "            else:\n",
        "                PROJECT_ID = \"default_project\"\n",
        "        else:\n",
        "            PROJECT_ID = \"default_project\"\n",
        "    else:\n",
        "        PROJECT_ID = \"0093\"  # fallback, change if needed\n",
        "\n",
        "if 'BASE_DIR' not in globals():\n",
        "    BASE_DIR = f\"/content/drive/MyDrive/kraken_projects/{PROJECT_ID}\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Train/val lists ‚Äì use existing vars if defined, otherwise defaults\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "_TR = TRAIN_LIST if 'TRAIN_LIST' in globals() else \"/content/alto_staged/train_local.txt\"\n",
        "_VA = VAL_LIST   if 'VAL_LIST'   in globals() else \"/content/alto_staged/val_local.txt\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Output directory for this recognizer attempt\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "REC_MODELS_DIR = os.path.join(BASE_DIR, \"models\", \"rec\")\n",
        "os.makedirs(REC_MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# ATTEMPT_ID normalization\n",
        "if 'ATTEMPT_ID' not in globals():\n",
        "    ATTEMPT_ID = \"attempt_01\"\n",
        "else:\n",
        "    if isinstance(ATTEMPT_ID, int):\n",
        "        ATTEMPT_ID = f\"attempt_{ATTEMPT_ID:02d}\"\n",
        "    else:\n",
        "        ATTEMPT_ID = str(ATTEMPT_ID)\n",
        "\n",
        "out_prefix = os.path.join(REC_MODELS_DIR, ATTEMPT_ID)\n",
        "\n",
        "print(\"üìÅ Project ID:     \", PROJECT_ID)\n",
        "print(\"üìÇ BASE_DIR:       \", BASE_DIR)\n",
        "print(\"üìÑ Train list:     \", _TR)\n",
        "print(\"üìÑ Val list:       \", _VA)\n",
        "print(\"üì¶ Output prefix:  \", out_prefix)\n",
        "print(\"üß† Base model:     \", BASE_MODEL if 'BASE_MODEL' in globals() else None)\n",
        "print()\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Build ketos train command\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "if 'LAG_EPOCHS' not in globals():\n",
        "    LAG_EPOCHS = 6\n",
        "if 'MIN_DELTA' not in globals():\n",
        "    MIN_DELTA = 0.0001\n",
        "if 'BATCH_SIZE' not in globals():\n",
        "    BATCH_SIZE = 8\n",
        "# LEARNING_RATE may be None ‚Üí kraken default\n",
        "\n",
        "cmd = [\n",
        "    \"ketos\", \"train\",\n",
        "    \"-o\", out_prefix,\n",
        "    \"-f\", \"alto\",\n",
        "    \"-t\", _TR,\n",
        "    \"-e\", _VA,\n",
        "    \"-q\", \"early\",\n",
        "    \"--lag\", str(int(LAG_EPOCHS)),\n",
        "    \"--min-delta\", str(float(MIN_DELTA)),\n",
        "    \"-B\", str(int(BATCH_SIZE)),\n",
        "    \"-F\", \"1\",\n",
        "]\n",
        "\n",
        "if 'BASE_MODEL' in globals() and BASE_MODEL:\n",
        "    cmd += [\"-i\", BASE_MODEL, \"--resize\", \"add\"]\n",
        "\n",
        "if 'LEARNING_RATE' in globals() and LEARNING_RATE is not None:\n",
        "    cmd += [\"-r\", str(LEARNING_RATE)]\n",
        "\n",
        "print(\"Running:\")\n",
        "print(textwrap.fill(\" \".join(shlex.quote(c) for c in cmd), width=120))\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Run training (interrupt-aware, line-by-line streaming)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "BEST_MODEL_PATH = None\n",
        "proc = None\n",
        "\n",
        "try:\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "    )\n",
        "\n",
        "    for line in proc.stdout:\n",
        "        print(line, end=\"\")\n",
        "\n",
        "    proc.stdout.close()\n",
        "    proc.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚õî Training interrupted by user. Trying to stop ketos‚Ä¶\")\n",
        "    if proc is not None and proc.poll() is None:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=10)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "finally:\n",
        "    best_candidate = out_prefix + \"_best.mlmodel\"\n",
        "    if os.path.exists(best_candidate):\n",
        "        BEST_MODEL_PATH = best_candidate\n",
        "        print(f\"\\nüåü Best model found at: {BEST_MODEL_PATH}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No _best.mlmodel found yet (did training reach at least one epoch?)\")\n",
        "\n",
        "    if proc is not None and proc.returncode not in (0, None):\n",
        "        print(f\"‚ö†Ô∏è ketos exited with return code {proc.returncode}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9) Download the best model\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "09lbLq5IT9hf"
      },
      "id": "09lbLq5IT9hf"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from ipywidgets import Dropdown, Button, VBox, Output\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Adjust this if your project path pattern ever changes ---\n",
        "if 'PROJECT_ID' not in globals():\n",
        "    PROJECT_ID = \"0093\"  # fallback; change if needed\n",
        "\n",
        "BASE_DIR = f\"/content/drive/MyDrive/kraken_projects/{PROJECT_ID}\"\n",
        "REC_MODELS_DIR = os.path.join(BASE_DIR, \"models\", \"rec\")\n",
        "\n",
        "if not os.path.isdir(REC_MODELS_DIR):\n",
        "    raise RuntimeError(f\"Recognizer models directory not found: {REC_MODELS_DIR}\")\n",
        "\n",
        "# Find all .mlmodel files in the recognizer directory\n",
        "all_models = [\n",
        "    f for f in os.listdir(REC_MODELS_DIR)\n",
        "    if f.endswith(\".mlmodel\")\n",
        "]\n",
        "\n",
        "if not all_models:\n",
        "    raise RuntimeError(\"No .mlmodel files found yet. Train at least one model first.\")\n",
        "\n",
        "# Prefer *_best.mlmodel if they exist; otherwise fall back to all models\n",
        "best_models = [f for f in all_models if f.endswith(\"_best.mlmodel\")]\n",
        "\n",
        "if best_models:\n",
        "    print(\"Found best models:\")\n",
        "    model_files = sorted(best_models)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No *_best.mlmodel files found; listing all models instead.\")\n",
        "    model_files = sorted(all_models)\n",
        "\n",
        "# Build dropdown options: label ‚Üí full path\n",
        "options = []\n",
        "for f in model_files:\n",
        "    label = f.replace(\".mlmodel\", \"\")  # e.g. \"attempt_01_best\"\n",
        "    full_path = os.path.join(REC_MODELS_DIR, f)\n",
        "    options.append((label, full_path))\n",
        "\n",
        "attempt_dropdown = Dropdown(\n",
        "    options=options,\n",
        "    description=\"Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "download_button = Button(\n",
        "    description=\"Download selected model\",\n",
        "    button_style=\"\",  # \"success\", \"info\", etc. if you like\n",
        ")\n",
        "\n",
        "out = Output()\n",
        "\n",
        "def on_download_clicked(b):\n",
        "    with out:\n",
        "        out.clear_output()\n",
        "        model_path = attempt_dropdown.value\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Downloading: {model_path}\")\n",
        "            files.download(model_path)\n",
        "        else:\n",
        "            print(f\"‚ùå File not found: {model_path}\")\n",
        "\n",
        "download_button.on_click(on_download_clicked)\n",
        "\n",
        "display(VBox([attempt_dropdown, download_button, out]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "b354b4215e0d4585bab194eb9a0f1dd6",
            "a0a5aa4e011d4696a0c7d2c49b1cb645",
            "1ba2d75a7756404a9c15465986f83654",
            "eb354e47cc384037a8be28b05cd20e0a",
            "6adccbd0ba1f492aa92739c847554d94",
            "0f9729248b194f5a99856e90b0cd1875",
            "aa4c18370b6b4aa7bbec59a57025adc6",
            "e88976a0d1784d98b72f42a87c10f013",
            "99b70b17ca6e428e832f04e779e97143",
            "2c2b41b44def4a129d43b9ec4d0ee480"
          ]
        },
        "id": "72Z3KqCRT64N",
        "outputId": "233e9545-0f60-486b-cb28-f97dee9ecc7e"
      },
      "id": "72Z3KqCRT64N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è No *_best.mlmodel files found; listing all models instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Model:', options=(('4210_flip_seg1', '/content/drive/MyDrive/kraken_proje‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b354b4215e0d4585bab194eb9a0f1dd6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8e8db5",
      "metadata": {
        "id": "8d8e8db5"
      },
      "source": [
        "## 9) Evaluate (CER/WER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0213fee2",
      "metadata": {
        "id": "0213fee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f97a6344-6e9a-4b8b-efc4-3a11be1490d0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_best' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2214403248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ketos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"alto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_best' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "import shlex, subprocess\n",
        "\n",
        "cmd = [\"ketos\", \"test\", \"-f\", \"alto\", \"-m\", final_best, VAL_LIST]\n",
        "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
        "res = subprocess.run(cmd, text=True)\n",
        "\n",
        "if res.returncode == 0:\n",
        "    print(\"‚úÖ Evaluation completed. See metrics above.\")\n",
        "else:\n",
        "    raise SystemExit(\"‚ùå Evaluation failed. Check the logs above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "501f4786",
      "metadata": {
        "id": "501f4786"
      },
      "source": [
        "\n",
        "### Notes\n",
        "- Change only `PROJECT_ID` in Project Settings; keep **everything else** under the project folder.\n",
        "- For each new attempt, adjust **ATTEMPT_ID**, **LEARNING_RATE**, and **BASE_SOURCE** in the **Training Controls** cell.\n",
        "- The best checkpoint for each attempt is copied to `attempt_XX.mlmodel` so you can chain attempts cleanly.\n",
        "- To warm-start a new manuscript from another one, keep **BASE_SOURCE = \"auto_other\"** for attempt 01 (or point to a model with `\"manual\"`).\n",
        "- For much faster training, switch to a **T4 GPU** in Colab and set `DEVICE = \"cuda\"`.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b354b4215e0d4585bab194eb9a0f1dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0a5aa4e011d4696a0c7d2c49b1cb645",
              "IPY_MODEL_1ba2d75a7756404a9c15465986f83654",
              "IPY_MODEL_eb354e47cc384037a8be28b05cd20e0a"
            ],
            "layout": "IPY_MODEL_6adccbd0ba1f492aa92739c847554d94"
          }
        },
        "a0a5aa4e011d4696a0c7d2c49b1cb645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "4210_flip_seg1"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_0f9729248b194f5a99856e90b0cd1875",
            "style": "IPY_MODEL_aa4c18370b6b4aa7bbec59a57025adc6"
          }
        },
        "1ba2d75a7756404a9c15465986f83654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Download selected model",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e88976a0d1784d98b72f42a87c10f013",
            "style": "IPY_MODEL_99b70b17ca6e428e832f04e779e97143",
            "tooltip": ""
          }
        },
        "eb354e47cc384037a8be28b05cd20e0a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2c2b41b44def4a129d43b9ec4d0ee480",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Downloading: /content/drive/MyDrive/kraken_projects/0093/models/rec/4210_flip_seg1.mlmodel\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Javascript object>",
                  "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "
                },
                "metadata": {}
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.Javascript object>",
                  "application/javascript": "download(\"download_26cbf1c6-7714-4152-ab69-5bffaef39261\", \"4210_flip_seg1.mlmodel\", 5067531)"
                },
                "metadata": {}
              }
            ]
          }
        },
        "6adccbd0ba1f492aa92739c847554d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f9729248b194f5a99856e90b0cd1875": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa4c18370b6b4aa7bbec59a57025adc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e88976a0d1784d98b72f42a87c10f013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b70b17ca6e428e832f04e779e97143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2c2b41b44def4a129d43b9ec4d0ee480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}