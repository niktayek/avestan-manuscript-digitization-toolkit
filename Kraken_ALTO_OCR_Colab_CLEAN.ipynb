{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade0c31f",
   "metadata": {},
   "source": [
    "\n",
    "# Kraken + ALTO OCR â€” Clean Colab (Nonâ€‘expert friendly)\n",
    "\n",
    "This notebook helps you **train and evaluate** a Kraken recognition model on ALTO XML data using Google Colab.\n",
    "\n",
    "**Whatâ€™s improved here**  \n",
    "- Fast setup: we **donâ€™t** install large packages on Drive. Instead, we use a **small pip cache on Drive** to make future installs much faster.  \n",
    "- Single place to edit paths: thereâ€™s **one Project Settings** cell with easy form fields.  \n",
    "- Stepâ€‘byâ€‘step explanations with clear instructions and checks.  \n",
    "- Removed unused/debug cells.\n",
    "\n",
    "> Tip: Run cells **top to bottom**. Each cell explains what it does.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10456e70",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Connect Google Drive\n",
    "\n",
    "This mounts your Drive at `/content/drive` so we can store training data, models, and the small pip cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive  # type: ignore\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Drive mounted at /content/drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65468ca7",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Project Settings (edit here once)\n",
    "\n",
    "Fill these fields. We will create folders automatically if they don't exist.\n",
    "\n",
    "- `PROJECT_ID`: a short name for your project (e.g., `0093`).  \n",
    "- `DATA_DIR`: where your ALTO data lives. It should contain **images** and **ALTO XMLs** in a structure your lists expect.  \n",
    "- `MODELS_DIR`: where trained models will be stored.  \n",
    "- `CPU_THREADS`: how many CPU threads to use.  \n",
    "- `DEVICE`: `\"cpu\"` is safe for everyone. If you enable GPU in Colab (**Runtime â†’ Change runtime type â†’ T4**), you can try `cuda`.\n",
    "\n",
    "> You can keep the defaults below and only change `PROJECT_ID`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ðŸ”§ Project Settings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"0093\"  #@param {type:\"string\"}\n",
    "ROOT_IN_DRIVE = \"/content/drive/MyDrive\"\n",
    "\n",
    "# Where your ALTO data is (images + XML). Change if needed.\n",
    "DATA_DIR = f\"{ROOT_IN_DRIVE}/kraken_data/{PROJECT_ID}\"  #@param {type:\"string\"}\n",
    "# Where models should be saved\n",
    "MODELS_DIR = f\"{ROOT_IN_DRIVE}/kraken_models/{PROJECT_ID}/rec\"  #@param {type:\"string\"}\n",
    "# Small persistent pip cache (fast re-installs without big space usage)\n",
    "PIP_CACHE_DIR = f\"{ROOT_IN_DRIVE}/.pip-cache\"  #@param {type:\"string\"}\n",
    "\n",
    "# Training config\n",
    "CPU_THREADS = 2  #@param {type:\"number\"}\n",
    "DEVICE = \"cpu\"   #@param [\"cpu\", \"cuda\"]\n",
    "\n",
    "# Derived paths (do not edit)\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PIP_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File lists (we'll create/check these later)\n",
    "LISTS_DIR = \"/content/lists\"\n",
    "Path(LISTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "TRAIN_LIST = f\"{LISTS_DIR}/train.txt\"\n",
    "VAL_LIST   = f\"{LISTS_DIR}/val.txt\"\n",
    "\n",
    "print(\"âœ… Settings applied\")\n",
    "print(\"PROJECT_ID:\", PROJECT_ID)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n",
    "print(\"PIP_CACHE_DIR:\", PIP_CACHE_DIR)\n",
    "print(\"LISTS_DIR:\", LISTS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba349c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Install Kraken (fast) â€” only if missing\n",
    "\n",
    "Colab machines are temporary. We can't permanently install system tools on Googleâ€™s side, but we can **speed up installs** by using a **small pip cache on Drive**.  \n",
    "- First time: takes a few minutes.  \n",
    "- Next times (with same account): much faster because cached wheels are re-used.\n",
    "\n",
    "This cell **checks if Kraken is already available**. If not, it installs it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc326481",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, subprocess, shlex\n",
    "\n",
    "# Persist pip cache on Drive (small space, big speed-up)\n",
    "os.environ[\"PIP_CACHE_DIR\"] = PIP_CACHE_DIR\n",
    "\n",
    "def is_importable(pkg: str) -> bool:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "need_install = not is_importable(\"kraken\")\n",
    "\n",
    "if not need_install:\n",
    "    import kraken\n",
    "    try:\n",
    "        import torch\n",
    "        torch_v = torch.__version__\n",
    "    except Exception:\n",
    "        torch_v = \"unknown\"\n",
    "    print(f\"âœ… Kraken already available (version: {getattr(kraken, '__version__', 'unknown')}) | Torch: {torch_v}\")\n",
    "else:\n",
    "    print(\"â³ Installing Kraken (this is only needed the first time on a new Colab VM)...\")\n",
    "    # Upgrade pip first (often speeds up dependency resolution)\n",
    "    subprocess.run(shlex.split(\"python -m pip -q install --upgrade pip\"), check=True)\n",
    "    # Install kraken (cairo extras enable PDF/ALTO pipelines)\n",
    "    # Torch will be pulled automatically with a compatible build\n",
    "    subprocess.run(shlex.split(\"python -m pip -q install 'kraken[cairo]'\"), check=True)\n",
    "\n",
    "    # Verify\n",
    "    import kraken, importlib\n",
    "    importlib.reload(kraken)\n",
    "    try:\n",
    "        import torch\n",
    "        torch_v = torch.__version__\n",
    "    except Exception:\n",
    "        torch_v = \"unknown\"\n",
    "    print(f\"âœ… Installed Kraken (version: {getattr(kraken, '__version__', 'unknown')}) | Torch: {torch_v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ff506",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Create or Validate Training Lists\n",
    "\n",
    "Krakenâ€™s `ketos train` expects lists of image/XML pairs.  \n",
    "If you **already have** `train.txt` and `val.txt`, place them in `/content/lists` and skip generation.  \n",
    "Otherwise, use the helper below to **auto-generate lists** by scanning `DATA_DIR`.\n",
    "\n",
    "**Expected format (one sample per line):**  \n",
    "```\n",
    "/path/to/image.png\t/path/to/alto.xml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: build simple lists by pairing image files and ALTO xmls with same stem.\n",
    "# Adjust patterns if your data layout differs.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"}\n",
    "XML_EXTS = {\".xml\"}\n",
    "\n",
    "def find_pairs(root: str) -> List[Tuple[str, str]]:\n",
    "    rootp = Path(root)\n",
    "    imgs = {}\n",
    "    xmls = {}\n",
    "    for p in rootp.rglob(\"*\"):\n",
    "        if p.suffix.lower() in IMG_EXTS:\n",
    "            imgs[p.stem] = str(p)\n",
    "        elif p.suffix.lower() in XML_EXTS:\n",
    "            # Heuristic: only include ALTO-like files (contains <alto> tag)\n",
    "            try:\n",
    "                # Fast check without fully parsing (safe for small files)\n",
    "                with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                    head = fh.read(4096)\n",
    "                    if \"<alto\" in head:\n",
    "                        xmls[p.stem] = str(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "    pairs = []\n",
    "    for k, img in imgs.items():\n",
    "        if k in xmls:\n",
    "            pairs.append((img, xmls[k]))\n",
    "    return pairs\n",
    "\n",
    "def write_list(pairs: List[Tuple[str, str]], out_path: str):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for img, xml in pairs:\n",
    "            f.write(f\"{img}\\t{xml}\\n\")\n",
    "\n",
    "AUTO_GENERATE = True  # change to False if you already prepared lists\n",
    "\n",
    "if AUTO_GENERATE:\n",
    "    pairs = find_pairs(DATA_DIR)\n",
    "    n = len(pairs)\n",
    "    if n < 2:\n",
    "        raise SystemExit(f\"âŒ Not enough samples in {DATA_DIR}. Found {n} image+ALTO pairs. \"\n",
    "                         f\"Please check your data structure.\")\n",
    "    # simple split 90/10\n",
    "    cut = max(1, int(n * 0.9))\n",
    "    train_pairs, val_pairs = pairs[:cut], pairs[cut:]\n",
    "    write_list(train_pairs, TRAIN_LIST)\n",
    "    write_list(val_pairs, VAL_LIST)\n",
    "    print(f\"âœ… Auto-generated lists: {len(train_pairs)} train, {len(val_pairs)} val\")\n",
    "else:\n",
    "    # Validate existing lists\n",
    "    for p in [TRAIN_LIST, VAL_LIST]:\n",
    "        if not Path(p).exists():\n",
    "            raise SystemExit(f\"âŒ Expected list not found: {p}\")\n",
    "    print(\"âœ… Using existing train/val lists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778ba6a",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Sanity Check\n",
    "\n",
    "We count samples and show a few example lines so you can quickly verify the lists are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import islice\n",
    "\n",
    "def count_lines(p):\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def head(p, n=5):\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        return list(islice(f, n))\n",
    "\n",
    "n_train = count_lines(TRAIN_LIST) if Path(TRAIN_LIST).exists() else 0\n",
    "n_val = count_lines(VAL_LIST) if Path(VAL_LIST).exists() else 0\n",
    "\n",
    "print(f\"Train samples: {n_train} | Val samples: {n_val}\")\n",
    "print(\"\\nSample train lines:\")\n",
    "print(\"\".join(head(TRAIN_LIST, 5)))\n",
    "print(\"Sample val lines:\")\n",
    "print(\"\".join(head(VAL_LIST, 3)))\n",
    "\n",
    "if n_train == 0:\n",
    "    raise SystemExit(\"âŒ No training samples found. Please fix your lists or data paths in the Project Settings cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7712838",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Train Recognition Model\n",
    "\n",
    "This uses `ketos train`. If you already have a base model (e.g., from a previous attempt), put it in `MODELS_DIR` and set its filename below to **optionally continue training**.\n",
    "\n",
    "- Outputs: intermediate checkpoints + the best model in `MODELS_DIR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, shlex, subprocess, glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: base model to continue from (leave empty to start from scratch)\n",
    "BASE_MODEL = \"\"  # e.g., f\"{MODELS_DIR}/attempt_01.mlmodel\"\n",
    "\n",
    "# Where to write the new model\n",
    "ATTEMPT_NAME = \"attempt_01\"  # change if you want to keep multiple attempts\n",
    "OUT_MODEL = f\"{MODELS_DIR}/{ATTEMPT_NAME}.mlmodel\"\n",
    "\n",
    "cmd = [\n",
    "    \"ketos\",\"train\",\n",
    "    \"-o\", OUT_MODEL,\n",
    "    \"--workers\", str(int(CPU_THREADS)),\n",
    "    \"--device\", DEVICE,\n",
    "    \"-f\", \"alto\",\n",
    "    TRAIN_LIST, VAL_LIST\n",
    "]\n",
    "\n",
    "if BASE_MODEL and Path(BASE_MODEL).exists():\n",
    "    cmd += [\"--load\", BASE_MODEL]\n",
    "\n",
    "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "result = subprocess.run(cmd, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"âœ… Training finished. Model at: {OUT_MODEL}\")\n",
    "else:\n",
    "    raise SystemExit(\"âŒ Training failed. Check the logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb638f08",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Evaluate the Model (CER/WER)\n",
    "\n",
    "Runs `ketos test` on the validation list to get quick metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f391507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "MODEL_FOR_EVAL = OUT_MODEL  # change if you want to evaluate another checkpoint\n",
    "\n",
    "cmd = [\"ketos\", \"test\", \"-f\", \"alto\", \"-m\", MODEL_FOR_EVAL, VAL_LIST]\n",
    "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "\n",
    "res = subprocess.run(cmd, text=True)\n",
    "if res.returncode == 0:\n",
    "    print(\"âœ… Evaluation completed. See metrics above.\")\n",
    "else:\n",
    "    raise SystemExit(\"âŒ Evaluation failed. Check the logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9a6b8",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (Optional) Run Inference on a Folder\n",
    "\n",
    "Use your trained model to transcribe a folder of ALTO image/XML pairs. Adjust inputs/outputs as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example inference command; adjust input/output folders to your setup.\n",
    "# For ALTO pipelines you typically use `ketos` or `kraken` CLI tools depending on your flow.\n",
    "# Here we just show a pattern; feel free to replace with your exact inference step.\n",
    "\n",
    "import shlex, subprocess, os\n",
    "INFERENCE_INPUT = DATA_DIR          # change if needed\n",
    "INFERENCE_OUTPUT = f\"{MODELS_DIR}/inference_out\"\n",
    "os.makedirs(INFERENCE_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(\"NOTE: Adjust this cell to your exact inference pipeline if needed.\")\n",
    "print(\"For ALTO workflows, you may prefer running `kraken` CLI directly for segmentation/recognition.\")\n",
    "\n",
    "# Example placeholder (no-op): list files we would process\n",
    "!ls -lah \"$INFERENCE_INPUT\" | head -n 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569fffc",
   "metadata": {},
   "source": [
    "\n",
    "## 9) FAQ / Troubleshooting\n",
    "\n",
    "**Q: Can we install Kraken once on the Colab system forever?**  \n",
    "A: Colab VMs are **temporary** and reset. We **cannot** persist system installs. The best workaround is what we use here: a **small pip cache on Drive** to make future installs **much faster** without storing a huge environment on Drive.\n",
    "\n",
    "**Q: I get `No training samples found ...`**  \n",
    "A: Make sure your `DATA_DIR` really contains image/XML pairs and that the list generator finds them. The sample generator expects **matching filenames** (e.g., `page001.png` with `page001.xml`) and the XML to include `<alto ...>`.\n",
    "\n",
    "**Q: How do I resume training from a previous checkpoint?**  \n",
    "A: Put your `.mlmodel` in `MODELS_DIR` and set `BASE_MODEL` to its path. The training cell will add `--load` automatically.\n",
    "\n",
    "**Q: Can I use GPU?**  \n",
    "A: Yes. In Colab, go to **Runtime â†’ Change runtime type â†’ T4 GPU**, then set `DEVICE = \"cuda\"` in Project Settings. If install fails, re-run the Install cell so Torch pulls a CUDA build.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
