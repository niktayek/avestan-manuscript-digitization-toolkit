{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc1baa8",
   "metadata": {},
   "source": [
    "# Kraken OCR — Colab Training (Setup Wizard)\n",
    "\n",
    "Run these three cells in order:\n",
    "1) Cell A — Setup Wizard\n",
    "2) Cell B — Build train/val lists\n",
    "3) Cell C — Train with checkpoints & save the BEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f590f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell A: Setup Wizard (run once per session) ===\n",
    "from google.colab import drive, files\n",
    "from pathlib import Path\n",
    "import json, os, sys, subprocess, importlib, zipfile, io, re, datetime\n",
    "\n",
    "PROJECT_NAME   = \"0093\"\n",
    "FORCE_ATTEMPT  = None\n",
    "PIN_KRAKEN_VER = \"\"\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "HOME     = Path(\"/content/drive/MyDrive\")\n",
    "ROOT     = HOME / \"kraken_projects\" / PROJECT_NAME\n",
    "ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG_P = ROOT / \"config.json\"\n",
    "DATA_ROOT_DEFAULT = ROOT / \"data\"\n",
    "MODELS_DIR        = ROOT / \"models\" / \"rec\"\n",
    "CKPTS_DIR_ROOT    = ROOT / \"ckpts\"\n",
    "PIP_CACHE_DIR     = HOME / \"pip-cache\"\n",
    "for d in [DATA_ROOT_DEFAULT, MODELS_DIR, CKPTS_DIR_ROOT, PIP_CACHE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def ensure(pkg_spec: str):\n",
    "    try:\n",
    "        mod_name = pkg_spec.split(\"[\" ,1)[0].split(\"==\",1)[0]\n",
    "        importlib.import_module(mod_name)\n",
    "        print(f\"✓ {pkg_spec} already available.\")\n",
    "    except Exception:\n",
    "        print(f\"Installing {pkg_spec} (cached in Drive) ...\")\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--no-input\",\n",
    "               \"--cache-dir\", str(PIP_CACHE_DIR), pkg_spec]\n",
    "        subprocess.check_call(cmd)\n",
    "\n",
    "kraken_spec = f\"kraken[train]{'=='+PIN_KRAKEN_VER if PIN_KRAKEN_VER else ''}\"\n",
    "ensure(kraken_spec)\n",
    "import kraken\n",
    "print(\"Kraken version:\", getattr(kraken, \"__version__\", \"unknown\"))\n",
    "try:\n",
    "    subprocess.run([\"ketos\", \"--version\"], check=False)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "cfg = {\n",
    "    \"project_name\": PROJECT_NAME,\n",
    "    \"data_dir\": str(DATA_ROOT_DEFAULT),\n",
    "    \"data_format\": None,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"random_seed\": 42,\n",
    "    \"models_dir\": str(MODELS_DIR),\n",
    "    \"ckpts_root\": str(CKPTS_DIR_ROOT),\n",
    "    \"attempt_num\": 1,\n",
    "    \"base_model_mode\": \"auto\",\n",
    "    \"base_model_manual\": \"\",\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 3e-4,\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "if CONFIG_P.exists():\n",
    "    try:\n",
    "        existing = json.loads(CONFIG_P.read_text())\n",
    "        if isinstance(existing, dict):\n",
    "            cfg.update(existing)\n",
    "        print(f\"Loaded existing config from {CONFIG_P}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not load existing config, starting fresh:\", e)\n",
    "\n",
    "print(\"\\nOPTIONAL: upload a ZIP dataset and/or a base .mlmodel for attempt_01 (you can also skip).\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "def pick_data_dir_for_zip(zip_name: str, default_root: Path):\n",
    "    stem = Path(zip_name).stem\n",
    "    return default_root / stem\n",
    "\n",
    "def detect_format_in_dir(d: Path):\n",
    "    xmls = list(d.rglob(\"*.xml\"))\n",
    "    imgs = []\n",
    "    for ext in [\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"]:\n",
    "        imgs += list(d.rglob(f\"*{ext}\"))\n",
    "    gts  = list(d.rglob(\"*.gt.txt\"))\n",
    "    if xmls:\n",
    "        sample = xmls[0]\n",
    "        try:\n",
    "            txt = sample.read_text(errors=\"ignore\")[:4096].lower()\n",
    "            if \"<alto\" in txt:\n",
    "                return \"alto\"\n",
    "            if \"<pcgts\" in txt or \"<page\" in txt:\n",
    "                return \"page\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        return \"alto\"\n",
    "    if imgs and gts:\n",
    "        return \"pairs\"\n",
    "    return None\n",
    "\n",
    "DATA_DIR = Path(cfg.get(\"data_dir\", DATA_ROOT_DEFAULT))\n",
    "\n",
    "for fname, blob in uploaded.items():\n",
    "    if fname.lower().endswith(\".zip\"):\n",
    "        target = pick_data_dir_for_zip(fname, DATA_ROOT_DEFAULT)\n",
    "        target.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Extracting ZIP to {target} ...\")\n",
    "        with zipfile.ZipFile(io.BytesIO(blob), 'r') as z:\n",
    "            z.extractall(target)\n",
    "        print(\"Done extracting.\")\n",
    "        DATA_DIR = target\n",
    "        cfg[\"data_dir\"] = str(DATA_DIR)\n",
    "    elif fname.lower().endswith(\".mlmodel\"):\n",
    "        dst = MODELS_DIR / \"attempt_01.mlmodel\"\n",
    "        if not dst.exists():\n",
    "            with open(dst, \"wb\") as f:\n",
    "                f.write(blob)\n",
    "            print(f\"Saved base model to {dst}\")\n",
    "        else:\n",
    "            print(f\"Base model already exists at {dst} (skipped).\")\n",
    "    else:\n",
    "        print(f\"Skipped {fname} (not .zip/.mlmodel)\")\n",
    "\n",
    "if not cfg.get(\"data_format\"):\n",
    "    df = detect_format_in_dir(DATA_DIR)\n",
    "    cfg[\"data_format\"] = df or \"pairs\"\n",
    "    print(\"Auto-detected DATA_FORMAT:\", cfg[\"data_format\"])\n",
    "\n",
    "def next_attempt(models_dir: Path):\n",
    "    nums = []\n",
    "    for p in models_dir.glob(\"attempt_*.mlmodel\"):\n",
    "        m = re.search(r\"attempt_(\\d+)\\.mlmodel$\", p.name)\n",
    "        if m:\n",
    "            nums.append(int(m.group(1)))\n",
    "    return (max(nums) + 1) if nums else 1\n",
    "\n",
    "if isinstance(FORCE_ATTEMPT, int) and FORCE_ATTEMPT > 0:\n",
    "    cfg[\"attempt_num\"] = FORCE_ATTEMPT\n",
    "else:\n",
    "    cfg[\"attempt_num\"] = next_attempt(MODELS_DIR)\n",
    "\n",
    "if cfg[\"attempt_num\"] <= 1:\n",
    "    base_model_path = MODELS_DIR / \"attempt_01.mlmodel\"\n",
    "else:\n",
    "    base_model_path = MODELS_DIR / f\"attempt_{cfg['attempt_num']-1:02d}.mlmodel\"\n",
    "\n",
    "if cfg.get(\"base_model_mode\") == \"manual\" and cfg.get(\"base_model_manual\"):\n",
    "    base_model_path = Path(cfg[\"base_model_manual\"])\n",
    "\n",
    "cfg[\"resolved_base_model\"] = str(base_model_path)\n",
    "\n",
    "CONFIG_P.write_text(json.dumps(cfg, ensure_ascii=False, indent=2))\n",
    "print(\"\\nSaved config to:\", CONFIG_P)\n",
    "print(json.dumps(cfg, ensure_ascii=False, indent=2))\n",
    "print(\"\\nNext: run Cell B — Build train/val lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell B: Build train/val lists (no edits) ===\n",
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "HOME     = Path(\"/content/drive/MyDrive\")\n",
    "ROOT     = HOME / \"kraken_projects\"\n",
    "\n",
    "def find_latest_config(root: Path):\n",
    "    configs = list(root.rglob(\"config.json\"))\n",
    "    if not configs:\n",
    "        raise SystemExit(\"No config.json found. Run Cell A (Setup Wizard) first.\")\n",
    "    configs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return configs[0]\n",
    "\n",
    "CONFIG_P = find_latest_config(ROOT)\n",
    "cfg = json.loads(CONFIG_P.read_text())\n",
    "print(\"Using config:\", CONFIG_P)\n",
    "\n",
    "DATA_DIR    = Path(cfg[\"data_dir\"])\n",
    "DATA_FORMAT = cfg[\"data_format\"]\n",
    "VAL_RATIO   = float(cfg.get(\"val_ratio\", 0.1))\n",
    "RANDOM_SEED = int(cfg.get(\"random_seed\", 42))\n",
    "\n",
    "LISTS_DIR = Path(\"/content/lists\")\n",
    "LISTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_list_path = LISTS_DIR / \"train.txt\"\n",
    "val_list_path   = LISTS_DIR / \"val.txt\"\n",
    "\n",
    "def collect_pairs(root: Path):\n",
    "    exts = {\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"}\n",
    "    images = []\n",
    "    for ext in exts:\n",
    "        images.extend(root.rglob(f\"*{ext}\"))\n",
    "    samples = []\n",
    "    for img in images:\n",
    "        base = img.with_suffix(\"\")\n",
    "        gt = img.parent / (base.name + \".gt.txt\")\n",
    "        if gt.exists():\n",
    "            samples.append(str(gt))\n",
    "    return sorted(set(samples))\n",
    "\n",
    "def collect_xml(root: Path):\n",
    "    return sorted(set(str(p) for p in root.rglob(\"*.xml\")))\n",
    "\n",
    "if DATA_FORMAT == \"pairs\":\n",
    "    all_samples = collect_pairs(DATA_DIR)\n",
    "elif DATA_FORMAT in (\"alto\", \"page\"):\n",
    "    all_samples = collect_xml(DATA_DIR)\n",
    "else:\n",
    "    raise SystemExit(\"DATA_FORMAT must be 'pairs', 'alto', or 'page'.\")\n",
    "\n",
    "if not all_samples:\n",
    "    pngs = sum(1 for _ in DATA_DIR.rglob(\"*.png\"))\n",
    "    gts  = sum(1 for _ in DATA_DIR.rglob(\"*.gt.txt\"))\n",
    "    xmls = sum(1 for _ in DATA_DIR.rglob(\"*.xml\"))\n",
    "    raise SystemExit(\n",
    "        f\"No training samples found under {DATA_DIR}.\\n\"\n",
    "        f\"DATA_FORMAT={DATA_FORMAT}\\n\"\n",
    "        f\"Found: {pngs} PNGs, {gts} .gt.txt files, {xmls} XML files.\\n\"\n",
    "        f\"If you uploaded ALTO/PAGE XML, set DATA_FORMAT='alto' or 'page' in Cell A.\\n\"\n",
    "        f\"If you have image+gt pairs, set DATA_FORMAT='pairs' and ensure *.gt.txt exist beside images.\"\n",
    "    )\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(all_samples)\n",
    "n = len(all_samples)\n",
    "n_val = max(1, int(n * VAL_RATIO))\n",
    "val = all_samples[:n_val]\n",
    "train = all_samples[n_val:]\n",
    "\n",
    "with open(train_list_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(train) + \"\\n\")\n",
    "with open(val_list_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(val) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(train)} train and {len(val)} val samples.\")\n",
    "print(\"Train list:\", train_list_path)\n",
    "print(\"Val   list:\", val_list_path)\n",
    "print(\"\\nNext: run Cell C — Train with checkpoints & save the BEST.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c151345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell C: Train with checkpoints & save the BEST (no edits) ===\n",
    "import json, re, csv, shutil, subprocess, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "HOME  = Path(\"/content/drive/MyDrive\")\n",
    "ROOT  = HOME / \"kraken_projects\"\n",
    "\n",
    "def find_latest_config(root: Path):\n",
    "    configs = list(root.rglob(\"config.json\"))\n",
    "    if not configs:\n",
    "        raise SystemExit(\"No config.json found. Run Cell A (Setup Wizard) first.\")\n",
    "    configs.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return configs[0]\n",
    "\n",
    "CONFIG_P = find_latest_config(ROOT)\n",
    "cfg = json.loads(CONFIG_P.read_text())\n",
    "print(\"Using config:\", CONFIG_P)\n",
    "\n",
    "DATA_FORMAT    = cfg[\"data_format\"]\n",
    "MODELS_DIR     = Path(cfg[\"models_dir\"])\n",
    "CKPTS_DIR_ROOT = Path(cfg[\"ckpts_root\"])\n",
    "ATTEMPT_NUM    = int(cfg[\"attempt_num\"])\n",
    "ATTEMPT_NAME   = f\"attempt_{ATTEMPT_NUM:02d}.mlmodel\"\n",
    "BEST_MODEL_OUT = MODELS_DIR / ATTEMPT_NAME\n",
    "BASE_MODEL     = Path(cfg.get(\"resolved_base_model\",\"\")) if cfg.get(\"resolved_base_model\") else None\n",
    "\n",
    "LISTS_DIR = Path(\"/content/lists\")\n",
    "train_list_path = LISTS_DIR / \"train.txt\"\n",
    "val_list_path   = LISTS_DIR / \"val.txt\"\n",
    "assert train_list_path.exists() and val_list_path.exists(), \"Run Cell B to build train/val lists first.\"\n",
    "\n",
    "CKPT_DIR = CKPTS_DIR_ROOT / f\"ckpts_{ATTEMPT_NAME.replace('.mlmodel','')}\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_opt = \"\"\n",
    "if BASE_MODEL and BASE_MODEL.exists():\n",
    "    base_opt = f'--load \"{BASE_MODEL}\"'\n",
    "    print(\"Using base model:\", BASE_MODEL)\n",
    "else:\n",
    "    print(\"No base model found. Training from scratch.\")\n",
    "\n",
    "EPOCHS     = int(cfg.get(\"epochs\", 20))\n",
    "LR         = float(cfg.get(\"lr\", 3e-4))\n",
    "BATCH_SIZE = int(cfg.get(\"batch_size\", 16))\n",
    "\n",
    "fmt_opt = \"\" if DATA_FORMAT == \"pairs\" else f\"-f {DATA_FORMAT}\"\n",
    "\n",
    "train_cmd = f'''ketos train {fmt_opt} {base_opt} --savefreq 1 --epochs {EPOCHS} -lr {LR} -b {BATCH_SIZE} -o \"{CKPT_DIR}/epoch_model.mlmodel\" $(cat \"{train_list_path}\") --validation $(cat \"{val_list_path}\")'''\n",
    "print(\"Training command:\\n\", train_cmd)\n",
    "\n",
    "ret = subprocess.call(train_cmd, shell=True, executable=\"/bin/bash\")\n",
    "if ret != 0:\n",
    "    print(\"Training exited non-zero (possibly interrupted). Proceeding to pick the best checkpoint.\")\n",
    "\n",
    "def parse_accuracy(output: str):\n",
    "    acc = None\n",
    "    cer = None\n",
    "    m = re.search(r'accuracy[:\\s]+([0-9.]+)%', output, re.I)\n",
    "    if m: acc = float(m.group(1))\n",
    "    m2 = re.search(r'CER[:\\s]+([0-9.]+)', output, re.I)\n",
    "    if m2: cer = float(m2.group(1))\n",
    "    return acc, cer\n",
    "\n",
    "def score_model(model_path: Path):\n",
    "    test_cmd = f'''ketos test -m \"{model_path}\" $(cat \"{val_list_path}\")'''\n",
    "    proc = subprocess.run(test_cmd, shell=True, executable=\"/bin/bash\",\n",
    "                          stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    out = proc.stdout\n",
    "    acc, cer = parse_accuracy(out)\n",
    "    score = acc if acc is not None else (100.0 - cer*100.0 if cer is not None else float(\"-inf\"))\n",
    "    return score, acc, cer\n",
    "\n",
    "ckpts = sorted(CKPT_DIR.glob(\"*.mlmodel\"))\n",
    "if not ckpts:\n",
    "    raise SystemExit(f\"No checkpoints found in {CKPT_DIR}. Nothing to pick.\")\n",
    "\n",
    "best = None\n",
    "for m in ckpts:\n",
    "    score, acc, cer = score_model(m)\n",
    "    print(f\"{m.name}: score={score:.4f}  acc={acc}  cer={cer}\")\n",
    "    if best is None or score > best[\"score\"]:\n",
    "        best = {\"path\": m, \"score\": score, \"acc\": acc, \"cer\": cer}\n",
    "\n",
    "shutil.copy2(best[\"path\"], BEST_MODEL_OUT)\n",
    "print(\"Saved best model as:\", BEST_MODEL_OUT)\n",
    "\n",
    "log_path = MODELS_DIR / \"attempts.csv\"\n",
    "is_new = not log_path.exists()\n",
    "with open(log_path, \"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if is_new:\n",
    "        w.writerow([\"timestamp\",\"attempt\",\"model_path\",\"base_model\",\"score\",\"accuracy\",\"cer\"])\n",
    "    w.writerow([datetime.datetime.now().isoformat(), ATTEMPT_NAME, str(BEST_MODEL_OUT),\n",
    "                str(BASE_MODEL) if BASE_MODEL else \"\", best[\"score\"], best[\"acc\"], best[\"cer\"]])\n",
    "\n",
    "print(\"Logged to:\", log_path)\n",
    "print(\"\\nDone. Next time, run Cell A again; it will auto-increment attempts and chain from the last saved model.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
