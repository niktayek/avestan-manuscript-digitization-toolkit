{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvXJ92UP8ZgH"
      },
      "outputs": [],
      "source": [
        "#@title init\n",
        "%%writefile __init__.py\n",
        "import copy\n",
        "serverconnections = [{'servername':'msIA',\n",
        "  'headers' : {'Authorization':'Token abe8934572784dea'},\n",
        "  'root_url' : 'https://msia.escriptorium.fr'},\n",
        " {'servername':'YOUR_SERVERNAME2',\n",
        "  'headers' : {'Authorization':'Token YOURTOKEN2'},\n",
        "  'root_url' : 'https://...'},\n",
        "  {'servername':'YOUR_SERVERNAME3',\n",
        "  'headers' : {'Authorization':'Token YOURTOKEN3'},\n",
        "  'root_url' : 'https://...'}]\n",
        "\n",
        "for serverconnection in serverconnections:\n",
        "    serverconnection['headersbrief'] = copy.deepcopy(serverconnection['headers'])\n",
        "    serverconnection['headers']['Content-type'] = 'application/json'\n",
        "    serverconnection['headers']['Accept']= 'application/json'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(\"<> Running the module as a script! <>\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title basic\n",
        "#%%writefile basic_api_functions.py\n",
        "\n",
        "# from basic_api_functions import create_new_document, create_transcription_levels, upload_image, loop_through_itempages, get_part_pk_list, get_all_lines_of_region,get_line_pk_list, get_linetranscription_pk_list, get_inhabited_region_pk_list, get_region_pk_list, get_region_box_list, get_page_transcription, get_document_segmentation_ontology, get_transcription_levels, get_set_of_all_characters_in_transcription_level, get_list_of_image_names, move_part, repolygonize, reorder,check_point_in_image,get_centroid,get_angle,rotateNP,line_intersection\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import io\n",
        "from requests.compat import urljoin\n",
        "from shapely.geometry import Point, Polygon\n",
        "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
        "from statistics import mean\n",
        "from PIL import Image\n",
        "from math import sqrt\n",
        "\n",
        "def search(pk, list_of_dictionaries):\n",
        "    result_list=[element for element in list_of_dictionaries if element['pk'] == pk]\n",
        "    if len(result_list)==1:\n",
        "      return result_list  [0]\n",
        "    elif len(result_list)==0:\n",
        "      return None\n",
        "    else:\n",
        "      return result_list\n",
        "\n",
        "def search_any(key, value, list_of_dictionaries):\n",
        "    return [element for element in list_of_dictionaries if element[key] == value]\n",
        "\n",
        "\n",
        "def get_idx(pk, list_of_dictionaries):\n",
        "  result_list=[n for n,element in enumerate(list_of_dictionaries) if element['pk'] == pk]\n",
        "  if len(result_list)==1:\n",
        "    return result_list[0]\n",
        "  elif len(result_list)==0:\n",
        "    return None\n",
        "  else:\n",
        "    return result_list\n",
        "\n",
        "def check_token_contains(key,chars,list_of_dictionaries):\n",
        "  return [n for n,element in enumerate(list_of_dictionaries) if any((c in chars) for c in element[key])]\n",
        "\n",
        "def get_serverinfo(servername,serverconnections):\n",
        "  serverconnection=search_any('servername', servername, serverconnections)[0]\n",
        "  print('switching to ',servername)\n",
        "  headers = serverconnection['headers']\n",
        "  headersbrief = serverconnection['headersbrief']\n",
        "  root_url = serverconnection['root_url']\n",
        "  return root_url,headers,headersbrief\n",
        "\n",
        "def dist2points(x1,y1,x2,y2):\n",
        "  dist=sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "  return dist\n",
        "\n",
        "def loop_through_itempages_and_get_all(base_url,suffix=''):\n",
        "    result_list=[]\n",
        "    def get_page(page,result_list):\n",
        "        data = False\n",
        "        #url = urljoin(base_url, '?page=%d' % page)\n",
        "        if suffix=='':\n",
        "          url = base_url+'?page='+str(page)\n",
        "        else:\n",
        "          url = base_url+'?page='+str(page)+'&'+suffix\n",
        "        res = requests.get(url, headers=headers)\n",
        "        #print(res.json())\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print('exception')\n",
        "            print(res)\n",
        "        else:\n",
        "          if data:\n",
        "         #   print(data)\n",
        "            result_list += [item for item in data['results']]\n",
        "        if data:\n",
        "          if data['next']:\n",
        "            get_page(page+1,result_list)\n",
        "    get_page(1,result_list)\n",
        "    return(result_list)\n",
        "\n",
        "def get_all_item_info(base_url):\n",
        "    result_list=[]\n",
        "    def get_page(page,result_list):\n",
        "        url = base_url+'?page='+str(page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            data = []\n",
        "            print(res)\n",
        "        else:\n",
        "            #print('empty')\n",
        "            #data = []\n",
        "            if data:\n",
        "              print(data)\n",
        "              result_list += [item for item in data['results']]\n",
        "        if data:\n",
        "          if (data['next']):\n",
        "            get_page(page+1,result_list)\n",
        "    get_page(1,result_list)\n",
        "    return(result_list)\n",
        "\n",
        "def find_string_in_page_transcription(doc_pk,part_pk,tr_level,query_string):\n",
        "    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)+'&'\n",
        "    r=requests.get(tr_url,headers=headers).json()\n",
        "    for n,line in enumerate(r['results']):\n",
        "      if line['content'].find(query_string)>-1:\n",
        "        print(part_pk,'line :',n)\n",
        "\n",
        "def get_basic_info(doc_pk):\n",
        "  '''\n",
        "  input: document ID\n",
        "  output:\n",
        "  part_list (list of all document parts),\n",
        "  transcription_level_list (list of different transcription levels)\n",
        "  region_type_list, line_type_list (segmentation ontologies)\n",
        "  '''\n",
        "  part_list=get_part_pk_list(doc_pk)\n",
        "  print('Document:', doc_pk,' with ',len(part_list),' parts')\n",
        "  region_type_list,line_type_list=get_document_segmentation_ontology(doc_pk)\n",
        "  transcription_level_list=get_transcription_levels(doc_pk)\n",
        "  return part_list,transcription_level_list,region_type_list,line_type_list\n",
        "\n",
        "def get_margin_and_paratext_type_pks(doc_pk):\n",
        "    region_type_list,line_type_list=get_document_segmentation_ontology(doc_pk)\n",
        "    for region_type in region_type_list:\n",
        "        if region_type['name']=='Margin':\n",
        "            margin_type_pk=region_type['pk']\n",
        "        elif region_type['name']=='Paratext':\n",
        "            paratext_type_pk=region_type['pk']\n",
        "    return margin_type_pk,paratext_type_pk\n",
        "\n",
        "def create_new_document(doc_name,script_pk=\"Hebrew\",read_direction=\"rtl\"):\n",
        "    url=root_url+'api/documents/'\n",
        "    data=json.dumps({\"name\": doc_name,\"main_script\": script_pk, \"read_direction\":read_direction,\"valid_block_types\": [{\"pk\": 3,\"name\": \"Commentary\"},{\"pk\": 4,\"name\": \"Illustration\"\n",
        "},{\"pk\": 2,\"name\": \"Main\"},{\"pk\": 1,\"name\": \"Title\"}]})\n",
        "    res = requests.post(url,headers=headers,data=data)\n",
        "    print(res.status_code, res.content)\n",
        "    jsonResponse=json.loads(res.content.decode('utf-8'))\n",
        "    doc_pk=jsonResponse['pk']\n",
        "    return doc_pk\n",
        "\n",
        "def create_transcription_levels(doc_pk,tr_name):\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/transcriptions/'\n",
        "    transcription_level_list = requests.get(url, headers=headers).json()\n",
        "    print('existing transcription levels: ',transcription_level_list)\n",
        "    datajson={'name':tr_name}\n",
        "    data=json.dumps(datajson)\n",
        "    res=requests.post(url,headers=headers,data=data)\n",
        "    jsonResponse=json.loads(res.content.decode('utf-8'))\n",
        "    tr_pk=jsonResponse['pk']\n",
        "    return tr_pk\n",
        "\n",
        "def upload_image(doc_pk,dirname,fname):\n",
        "\n",
        "    url = root_url+'api/documents/'+str(doc_pk)+'/parts/'\n",
        "#mydir=(r\"A:\\openITI\\testset\")\n",
        "#myfile='15.png'\n",
        "    file = os.path.join(dirname, fname)\n",
        "    with open(file, 'rb') as fh:\n",
        "        data={'name':fname}\n",
        "        res = requests.post(url,data=data, files={'image': fh},headers=headers)\n",
        "    print(res.status_code, res.content)\n",
        "    jsonResponse=json.loads(res.content.decode('utf-8'))\n",
        "    part_pk=jsonResponse['pk']\n",
        "    return part_pk\n",
        "\n",
        "def page_height_width(doc_pk,part_pk):\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  pagewidth=res['image']['size'][0]\n",
        "  pageheight=res['image']['size'][1]\n",
        "  return pageheight,pagewidth\n",
        "\n",
        "def get_img_from_url(img_url):\n",
        "  res=requests.get(img_url)\n",
        "  img = Image.open(io.BytesIO(res.content))\n",
        "  return(img)\n",
        "\n",
        "def rotate_img(doc_pk,part_pk,angle):\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/rotate/'\n",
        "  data={'angle':angle}\n",
        "  print(data)\n",
        "  res=requests.post(url=url,headers=headers,data=json.dumps(data))\n",
        "  print(res)\n",
        "\n",
        "def get_doc_list_from_instance():\n",
        "  doc_url=urljoin(root_url, '/api/documents/')\n",
        "  doc_info=requests.get(doc_url,headers=headers).json()\n",
        "  doc_pk_list=loop_through_itempages(doc_url,'pk','results')\n",
        "  doc_name_list=loop_through_itempages(doc_url,'name','results')\n",
        "  doc_img_nu_list=loop_through_itempages(doc_url,'parts_count','results')\n",
        "  f = open(\"results.tsv\",mode='wt')\n",
        "  for n,doc_pk in enumerate(doc_pk_list):\n",
        "    doc_name=doc_name_list[n]\n",
        "    doc_img_nu=doc_img_nu_list[n]\n",
        "    f.write(str(doc_pk)+'\\t'+doc_name+'\\t'+str(doc_img_nu)+'\\n')\n",
        "  f.close()\n",
        "\n",
        "def get_part_pk_list(doc_pk):\n",
        "    parts_url=urljoin(root_url, '/api/documents/%d/parts/' % int(doc_pk))\n",
        "    return(loop_through_itempages(parts_url,'pk','results'))\n",
        "\n",
        "def get_imgname_list(doc_pk):\n",
        "    parts_url=urljoin(root_url, '/api/documents/%d/parts/' % int(doc_pk))\n",
        "    return(loop_through_itempages(parts_url,'filename','results'))\n",
        "\n",
        "def get_line_pk_list(doc_pk,part_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    return(loop_through_itempages(lines_url,'pk','results'))\n",
        "\n",
        "def get_all_baselines_of_part(doc_pk,part_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    return(loop_through_itempages(lines_url,'baseline','results'))\n",
        "\n",
        "def get_linetranscription_pk_list(doc_pk, part_pk,tr_pk):\n",
        "    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
        "    return(loop_through_itempages(tr_url,'pk','results'))\n",
        "\n",
        "def get_inhabited_region_pk_list(doc_pk,part_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    return(list(set(loop_through_itempages(lines_url,'region','results'))))\n",
        "\n",
        "def get_region_pk_list(doc_pk,part_pk):\n",
        "    regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "    return(loop_through_itempages(regions_url,'pk','results'))\n",
        "\n",
        "\n",
        "def get_region_pk_list_of_type(doc_pk,part_pk,region_type):\n",
        "    regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "    region_list=[]\n",
        "    def get_page(page,region_list):\n",
        "        url = urljoin(regions_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            region_list += [region['pk'] for region in data['results'] if region['typology']==region_type]\n",
        "        if data['next']:\n",
        "            get_page(page+1,region_list)\n",
        "    get_page(1,region_list)\n",
        "    return(region_list)\n",
        "\n",
        "def get_regions_and_lines(doc_pk,part_pk,tr_level):\n",
        "  lines_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "  lines=loop_through_itempages_and_get_all(lines_url)\n",
        "  regions_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "  regions=loop_through_itempages_and_get_all(regions_url)\n",
        "  tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
        "  tr_suffix = 'transcription='+str(tr_level)\n",
        "  #transcriptions=requests.get(tr_url,headers=headers).json()\n",
        "  transcriptions=loop_through_itempages_and_get_all(tr_url,tr_suffix)\n",
        "  return regions,lines,transcriptions\n",
        "\n",
        "def get_main_region_pk_list(doc_pk,part_pk):\n",
        "    regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "    region_list=[]\n",
        "    def get_page(page,region_list):\n",
        "        url = urljoin(regions_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            region_list += [region['pk'] for region in data['results'] if region['typology']==2]\n",
        "        if data['next']:\n",
        "            get_page(page+1,region_list)\n",
        "    get_page(1,region_list)\n",
        "    return(region_list)\n",
        "\n",
        "def loop_through_itempages(base_url,query1,query2):\n",
        "    result_list=[]\n",
        "    def get_page(page,result_list):\n",
        "        data=None\n",
        "        url = base_url+ '?page='+str(page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        #print(res.json())\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "          if data and not(data=={'detail': 'You do not have permission to perform this action.'}):\n",
        "         #   print(data)\n",
        "            result_list += [item[query1] for item in data[query2]]\n",
        "        if data and not(data=={'detail': 'You do not have permission to perform this action.'}):\n",
        "          if data['next']:\n",
        "            get_page(page+1,result_list)\n",
        "    get_page(1,result_list)\n",
        "    return(result_list)\n",
        "\n",
        "def get_region_box_list(doc_pk,part_pk):\n",
        "    regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "    return(loop_through_itempages(regions_url,'box','results'))\n",
        "\n",
        "def get_page_transcription(doc_pk,part_pk,tr_level):\n",
        "    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)+'&'\n",
        "    return(loop_through_itempages(tr_url,'content','results'))\n",
        "\n",
        "def get_page_transcription_line_pks(doc_pk,part_pk,tr_level):\n",
        "    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)+'&'\n",
        "    return(loop_through_itempages(tr_url,'line','results'))\n",
        "\n",
        "def get_all_lines_of_region(doc_pk,part_pk,region_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    line_list=[]\n",
        "    def get_page(page,line_list):\n",
        "        url = urljoin(lines_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            line_list += [[line['pk'],line['order'],line['baseline'],line['typology']] for line in data['results'] if line['region']==region_pk]\n",
        "        if data['next']:\n",
        "            get_page(page+1,line_list)\n",
        "    get_page(1,line_list)\n",
        "    return(line_list)\n",
        "\n",
        "def get_all_line_pks_of_region(doc_pk,part_pk,region_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    line_list=[]\n",
        "    def get_page(page,line_list):\n",
        "        url = urljoin(lines_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            line_list += [line['pk'] for line in data['results'] if line['region']==region_pk]\n",
        "        if data['next']:\n",
        "            get_page(page+1,line_list)\n",
        "    get_page(1,line_list)\n",
        "    return(line_list)\n",
        "\n",
        "\n",
        "def get_all_lines_of_part(doc_pk,part_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    line_list=[]\n",
        "    def get_page(page,line_list):\n",
        "        url = urljoin(lines_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            line_list += [[line['pk'],line['order'],line['baseline'],line['typology'],line['region']] for line in data['results']]\n",
        "        if data['next']:\n",
        "            get_page(page+1,line_list)\n",
        "    get_page(1,line_list)\n",
        "    return(line_list)\n",
        "\n",
        "def get_all_full_lines_of_part(doc_pk,part_pk):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    line_info=[]\n",
        "    def get_page(page,line_info):\n",
        "        url = urljoin(lines_url, '?page=%d' % page)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        try:\n",
        "            data = res.json()\n",
        "        except json.decoder.JSONDecodeError as e:\n",
        "            print(res)\n",
        "        else:\n",
        "            line_info += data['results']\n",
        "        if data['next']:\n",
        "            get_page(page+1,line_info)\n",
        "    get_page(1,line_info)\n",
        "    return(line_info)\n",
        "\n",
        "def get_document_segmentation_ontology(doc_pk):\n",
        "    print('get document segmentation ontology for document: ',doc_pk)\n",
        "    document_base_json=requests.get(root_url+'api/documents/'+str(doc_pk)+'/',headers=headers).json()\n",
        "    print(document_base_json)\n",
        "    region_type_list=document_base_json['valid_block_types']\n",
        "    line_type_list=document_base_json['valid_line_types']\n",
        "    margin_type_pk=-1\n",
        "    paratext_type_pk=-1\n",
        "    for region_type in region_type_list:\n",
        "        print(region_type)\n",
        "    for line_type in line_type_list:\n",
        "        print(line_type)\n",
        "    return region_type_list,line_type_list\n",
        "\n",
        "def get_transcription_levels(doc_pk):\n",
        "    print('get transcription levels for document: ',doc_pk)\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/transcriptions/'\n",
        "    transcription_level_list = requests.get(url, headers=headers).json()\n",
        "    print(transcription_level_list)\n",
        "    return transcription_level_list\n",
        "\n",
        "def get_page_transcription_tr_pks(doc_pk,part_pk,tr_level):\n",
        "    tr_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)+'&'\n",
        "    return(loop_through_itempages(tr_url,'pk','results'))\n",
        "\n",
        "def get_set_of_all_characters_in_transcription_level(doc_pk,part_list,tr_level):\n",
        "    print('get char set of document for document: ',doc_pk)\n",
        "    chars = set()\n",
        "    for n,part_pk in enumerate(part_list):\n",
        "        if n % 100 == 0:\n",
        "            print(n,' parts finished')\n",
        "        transcriptions_this_part=get_page_transcription(doc_pk,part_pk,tr_level)\n",
        "        for line in transcriptions_this_part:\n",
        "            chars=chars.union(set(line))\n",
        "    chars=list(chars)\n",
        "    return chars\n",
        "\n",
        "def get_list_of_image_names(doc_pk,part_list):\n",
        "    print('order part_pk imgname')\n",
        "    for n,part_pk in enumerate(part_list):\n",
        "        url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)\n",
        "        res = requests.get(url,headers=headers).json()\n",
        "    #print('done')\n",
        "        print(n+1,part_pk,res['filename'])\n",
        "    print('done with getting imagefilename list')\n",
        "\n",
        "def move_part(doc_pk,part_pk,place):\n",
        "    print('move part_pk '+str(part_pk)+'to place '+str(place))\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/move/'\n",
        "    data='{\"index\":'+str(place)+'}'\n",
        "    res=requests.post(url,headers=headers,data=data)\n",
        "    print(res)\n",
        "\n",
        "def repolygonize(doc_pk,part_pk):\n",
        "    print('repolygonize : ',part_pk)\n",
        "    parts_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)\n",
        "    url_repoly=urljoin(parts_url,'/reset_masks/')\n",
        "    repoly_res=requests.post(url_repoly,headers=headers)\n",
        "    print('done repolygonizing')\n",
        "\n",
        "def repolygonize_line(doc_pk,part_pk,line_pk):\n",
        "    print('repolygonize line : ',line_pk)\n",
        "    parts_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)\n",
        "    url_repoly=parts_url+'/reset_masks/?only='+str(line_pk)\n",
        "    repoly_res=requests.post(url_repoly,headers=headers)\n",
        "    print('done repolygonizing')\n",
        "    return repoly_res\n",
        "\n",
        "def reorder(doc_pk,part_pk):\n",
        "    print('reorder ',part_pk)\n",
        "    reorder_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/recalculate_ordering/'\n",
        "    reorder_res = requests.post(reorder_url,headers=headers)\n",
        "    print('done reordering')\n",
        "\n",
        "def check_point_in_image(p,imgHeight,imgWidth,safetydistance):\n",
        "    # deal with x:\n",
        "    p[0]=min(imgWidth-safetydistance,max(p[0],safetydistance))\n",
        "    p[1]=min(imgHeight-safetydistance,max(p[1],safetydistance))\n",
        "    return p\n",
        "\n",
        "def get_centroid(line):\n",
        "    return [line[0,0]+line[-1][0],line[0,1]+line[-1][1]]\n",
        "\n",
        "def get_angle(line):\n",
        "    return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
        "\n",
        "def get_vector(angle,length):\n",
        "  #from numpy import rad2deg,deg2rad,cos,sin\n",
        "  #return rad2deg(arctan2(line[-1][-1] - line[0][-1], line[-1][0] - line[0][0]))\n",
        "  #length=hypothenuse\n",
        "  #ankathete=x\n",
        "  #gegenkathete=y\n",
        "  y=int(round(sin(deg2rad(angle))*length,0))\n",
        "  x=int(round(cos(deg2rad(angle))*length,0))\n",
        "  return x,y\n",
        "\n",
        "def check_baseline(doc_pk,part_pk,mindist):\n",
        "  res = requests.get(url='https://www.escriptorium.fr/api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/',headers=headers).json()\n",
        "  imgWidth,imgHeight = res['image']['size']\n",
        "  lines=res['lines']\n",
        "  change = False\n",
        "  for n,line in enumerate(lines):\n",
        "    baselinepoints = line['baseline']\n",
        "    for point in baselinepoints:\n",
        "      if point[0] < 1:\n",
        "        change = True\n",
        "        print(part_pk,n,'x', point[0])\n",
        "        res['lines'][n]['lines'][n][0] = mindist\n",
        "      if point[0] >= imgWidth:\n",
        "        change = True\n",
        "        print(part_pk,n,'x', point[0])\n",
        "        res['lines'][n][0] = imgWidth-mindist\n",
        "      if point[1] < 1:\n",
        "        change = True\n",
        "        print(part_pk,n,'y', point[1])\n",
        "        res['lines'][n][1] = mindist\n",
        "      if point[1] >= imgHeight:\n",
        "        change = True\n",
        "        print(part_pk,n,'y', point[1])\n",
        "        res['lines'][n][1] = imgHeight-mindist\n",
        "  return res\n",
        "\n",
        "\n",
        "def rotateNP(p, origin=(0, 0), degrees=0):\n",
        "    angle = deg2rad(degrees)\n",
        "    R = array([[cos(angle), -sin(angle)],\n",
        "                  [sin(angle),  cos(angle)]])\n",
        "    o = atleast_2d(origin)\n",
        "    p = atleast_2d(p)\n",
        "    return squeeze((R @ (p.T-o.T) + o.T).T)\n",
        "\n",
        "\n",
        "def line_intersection(line1, line2):\n",
        "    xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
        "    ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
        "\n",
        "    def det(a, b):\n",
        "        return a[0] * b[1] - a[1] * b[0]\n",
        "\n",
        "    div = det(xdiff, ydiff)\n",
        "    if div == 0:\n",
        "        print('lines do not intersect')\n",
        "        x = line1[0][0]\n",
        "        y = 100000\n",
        "    else:\n",
        "        d = (det(*line1), det(*line2))\n",
        "        x = det(d, xdiff) / div\n",
        "        y = det(d, ydiff) / div\n",
        "    return x, y\n",
        "\n",
        "def rename_tr_level(doc_pk,tr_level,new_name):\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/transcriptions/'+str(tr_level)+'/'\n",
        "  rjson=requests.get(url,headers=headers).json()\n",
        "  rjson['name']=new_name\n",
        "  r=requests.put(url,headers=headers,data=json.dumps(rjson))\n",
        "  return r\n",
        "\n",
        "def simplify_line_polygons_on_part(doc_pk,part_pk,simplification=10):\n",
        "  lines_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "  lines=loop_through_itempages_and_get_all(lines_url)\n",
        "  for line in lines:\n",
        "    mask=line['mask']\n",
        "    #print(len(mask),mask)\n",
        "    maskPoly=Polygon(mask)\n",
        "    maskSimple=maskPoly.simplify(10)\n",
        "    #print(maskSimple)\n",
        "    x,y = maskSimple.exterior.coords.xy\n",
        "    new_mask = [list(x) for x in zip(x,y)]\n",
        "    #print(len(new_mask),new_mask)\n",
        "    line_url = lines_url+str(line['pk'])+'/'\n",
        "    line_json={'mask' : new_mask}\n",
        "    res_line = requests.patch(line_url,headers=headers,data=json.dumps(line_json))\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "  #  print(\"<> Running the module as a script! <>\")\n",
        "    #get_transcription_levels(doc_pk)\n"
      ],
      "metadata": {
        "id": "G5LBNexf8wB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title complex\n",
        "#%%writefile complex_api_functions.py\n",
        "\n",
        "# find_and_replace_text_or_trim, transfer_txt2level, get_pages_without_transcription_level, get_pages_with_wordstring_in_transcription_level,\n",
        "# delete_linefree_regions, keep_only_biggest_region, associate_lines_with_existing_regions, delete_unlinked_lines, delete_1p_lines,\n",
        "# find_lines_without_mask, extend_lines, create_transcription_table, delete_empty_lines\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "from requests.compat import urljoin\n",
        "from shapely.geometry import Point, Polygon\n",
        "#from basic_api_functions import *\n",
        "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
        "from statistics import mean\n",
        "from PIL import Image\n",
        "from time import sleep\n",
        "\n",
        "def find_and_replace_text_or_trim(doc_pk,part_pk,tr_level,chars,replacement_char,do_replace,do_trim):\n",
        "  return_part_pk=''\n",
        "  if do_replace or do_trim:\n",
        "    if do_replace:\n",
        "      print('replacing '+chars+' with '+replacement_char)\n",
        "    if do_trim:\n",
        "      print('trim')\n",
        "  else:\n",
        "    print('find pages with '+chars)\n",
        "    chars=set(chars)\n",
        "  page_nu=0\n",
        "  while True:\n",
        "    page_nu+=1\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(tr_level)\n",
        "    #print('checking: '+root_url+'document/'+str(doc_pk)+'/part/'+str(part_pk)+'/edit/')\n",
        "    res_transcriptions_this_part = requests.get(url,headers=headers).json()\n",
        "    #print(res_transcriptions_this_part)\n",
        "    if not(do_replace or do_trim):\n",
        "      for m,line in enumerate(res_transcriptions_this_part['results']):\n",
        "        linestr=line['content']\n",
        "        if any((c in chars) for c in linestr):\n",
        "          print('have a look for character in ',part_pk,' line :',m)\n",
        "          return_part_pk=part_pk\n",
        "    else:\n",
        "      for m,line in enumerate(res_transcriptions_this_part['results']):\n",
        "        oldstr=line['content']\n",
        "        if ((do_replace and any((c in chars) for c in oldstr)) or do_trim):\n",
        "          t_url=(root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+str(line['pk'])+'/')\n",
        "          res_t=requests.get(t_url,headers=headers).json()\n",
        "          content=res_t['content']\n",
        "          #print('old:',content)\n",
        "          content=content.replace(chars,replacement_char)\n",
        "          #print('new:',content)\n",
        "          if do_trim:\n",
        "            content=content.strip()\n",
        "          if (do_replace or do_trim) and not(oldstr==content):\n",
        "            res_t['content']=content\n",
        "            data=json.dumps(res_t, ensure_ascii=False).encode('utf8')\n",
        "            res_put=requests.put(t_url,headers=headers,data=data)\n",
        "    if not(res_transcriptions_this_part['next']):\n",
        "      break\n",
        "  return return_part_pk\n",
        "\n",
        "def transfer_txt2level(doc_pk,part_pk,source_tr_level,target_tr_level):\n",
        "  target_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
        "  page_nu=0\n",
        "  while True:\n",
        "    page_nu+=1\n",
        "    source_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(source_tr_level)\n",
        "    #print(source_tr_url)\n",
        "    res = requests.get(source_tr_url, headers=headers).json()\n",
        "    for line in res['results']:\n",
        "      line_pk=line['line']\n",
        "      line_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'+str(line_pk)\n",
        "      #print(line_url)\n",
        "      res_line=requests.get(line_url,headers=headers).json()\n",
        "      #print(res_line)\n",
        "      content=line['content']\n",
        "      if not(res_line['typology']==3):\n",
        "         data = {'line': line_pk, 'transcription': target_tr_level, 'content': content}\n",
        "         res_post = requests.post(target_tr_url, data=json.dumps(data), headers=headers)\n",
        "    if not(res['next']):\n",
        "      break\n",
        "\n",
        "def transfer_txt2level_overwrite(doc_pk,part_pk,source_tr_level,target_tr_level):\n",
        "  page_nu=0\n",
        "  while True:\n",
        "    page_nu+=1\n",
        "    source_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(source_tr_level)\n",
        "    print(source_tr_url)\n",
        "    res1 = requests.get(source_tr_url, headers=headers).json()\n",
        "    target_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(target_tr_level)\n",
        "    res2 = requests.get(target_tr_url, headers=headers).json()\n",
        "    tg_line_pk_list = [line.get('line') for line in res2['results']]\n",
        "    for line in res1['results']:\n",
        "      line_pk=line['line']\n",
        "      content=line['content']\n",
        "      #print(line)\n",
        "      try:\n",
        "        tg_line_pk=res2['results'][tg_line_pk_list.index(line_pk)]['pk']\n",
        "        data = json.dumps({'line': line_pk, 'transcription': target_tr_level, 'content': content})\n",
        "        target_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'+str(tg_line_pk)+'/'\n",
        "        res_tg = requests.put(target_tr_url, data=data, headers=headers)\n",
        "      except:\n",
        "        print('not found: ', line_pk, '--> posted')\n",
        "        data = json.dumps({'line': line_pk, 'transcription': target_tr_level, 'content': content})\n",
        "        target_tr_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/'\n",
        "        res_tg = requests.post(target_tr_url, data=data, headers=headers)\n",
        "      #print(res_tg)\n",
        "    if not(res1['next']):\n",
        "      break\n",
        "\n",
        "def get_lines_without_transcription_level(doc_pk,part_pk,tr_level,also_empty_lines=False):\n",
        "  print('check pages without transcription in transcription level for pk', part_pk)\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)\n",
        "  res_transcriptions_this_part = requests.get(url,headers=headers).json()\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)\n",
        "  res_part = requests.get(url,headers=headers).json()\n",
        "  line_pk_list_this_part = [line.get('pk') for line in res_part['lines']]\n",
        "  line_pk_list_this_part_set=set(line_pk_list_this_part)\n",
        "  line_pk_list_this_transcription = [tr.get('line') for tr in res_transcriptions_this_part['results']]\n",
        "  line_pk_list_this_transcription_set = set(line_pk_list_this_transcription)\n",
        "  missing=line_pk_list_this_part_set.difference(line_pk_list_this_transcription_set)\n",
        "  if len(line_pk_list_this_part)>0:\n",
        "    proportion=len(missing)/len(line_pk_list_this_part)\n",
        "  else:\n",
        "    proportion=0\n",
        "  empty_lines=[]\n",
        "  if also_empty_lines:\n",
        "    for line in res_transcriptions_this_part['results']:\n",
        "      if line['content']=='':\n",
        "        empty_lines.append(line['pk'])\n",
        "  return missing,proportion,empty_lines\n",
        "\n",
        "def get_pages_with_wordstring_in_transcription_level(doc_pk,part_list,tr_level,words):\n",
        "## check which pages contain any given string in a specific transcription level part 2\n",
        "  print('check pages with string in transcription level')\n",
        "  for n,part_pk in enumerate(part_list):\n",
        "    if n % 50 == 0:\n",
        "      print(n)\n",
        "    transcriptions_this_part=get_page_transcription(doc_pk,part_pk,tr_level)\n",
        "    for m,line in enumerate(transcriptions_this_part):\n",
        "      linewords=line.split()\n",
        "      #print(linewords)\n",
        "      if any((w in words) for w in linewords):\n",
        "        print('check part '+str(n+1)+' line: '+str(m+1))\n",
        "        oldstr=line\n",
        "        print(oldstr)\n",
        "        print(root_url+'document/'+str(doc_pk)+'/part/'+str(part_pk)+'/edit/')\n",
        "  print('done')\n",
        "\n",
        "def find_or_delete_linefree_regions(doc_pk,part_pk,do_delete=False):\n",
        "\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  print(parts_url)\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  inhabited_region_list=set(get_inhabited_region_pk_list(doc_pk,part_pk))\n",
        "  region_list=set(get_region_pk_list(doc_pk,part_pk))\n",
        "  uninhabited_region_list=region_list.difference(inhabited_region_list)\n",
        "  for region in uninhabited_region_list:\n",
        "    print('found linefree region')\n",
        "    if do_delete:\n",
        "      delete_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/blocks/'+str(region)\n",
        "      r = requests.delete(delete_url,headers=headers)\n",
        "\n",
        "def keep_only_biggest_region(doc_pk,part_pk):\n",
        "\n",
        "  region_list = get_region_pk_list(doc_pk,part_pk)\n",
        "  biggest_area=0\n",
        "  biggest_area_pk=0\n",
        "  biggest_area_n=0\n",
        "  all_regions=[]\n",
        "  for m,region_nu in enumerate(region_list):\n",
        "    part_block_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/blocks/'+str(region_nu)+'/'\n",
        "    region = requests.get(part_block_url,headers=headers).json()\n",
        "    all_regions.append(region)\n",
        "    region_polygon=Polygon(region['box'])\n",
        "    if biggest_area<region_polygon.area:\n",
        "      biggest_area=region_polygon.area\n",
        "      biggest_area_pk=region['pk']\n",
        "      keep_region=region\n",
        "      keep_region['order']=0\n",
        "  part_block_url =root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/blocks/?format=json'\n",
        "  for n,region in enumerate(all_regions):\n",
        "    if not(region['pk']==biggest_area_pk):\n",
        "      print('delete '+str(region['pk']))\n",
        "      delete_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/blocks/'+str(region['pk']) + '/'\n",
        "      r = requests.delete(delete_url,headers=headers)\n",
        "      print(r.status_code)\n",
        "\n",
        "def associate_lines_with_existing_regions(doc_pk,part_pk):\n",
        "  print(part_pk)\n",
        "  line_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/'\n",
        "  region_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/blocks/'\n",
        "  lines=get_all_item_info(line_url)\n",
        "  regions=get_all_item_info(region_url)\n",
        "  nu_regions=len(regions)\n",
        "  region_pk_list = [item['pk'] for item in regions]\n",
        "  region_polygons = [item['box'] for item in regions]\n",
        "  line_pk_list = [item['pk'] for item in lines]\n",
        "  baseline_coords_list = [item['baseline'] for item in lines]\n",
        "  print('associating')\n",
        "  for n,line_pk in enumerate(line_pk_list):\n",
        "    baseline=baseline_coords_list[n]\n",
        "    centroidx=int(round((baseline[0][0]+baseline[-1][0])/2))\n",
        "    centroidy=int(round((baseline[0][1]+baseline[-1][1])/2))\n",
        "    p = Point(centroidx, centroidy)\n",
        "    region_nu = 0\n",
        "    while (region_nu<nu_regions):\n",
        "      poly = Polygon(region_polygons[region_nu])\n",
        "      if p.within(poly):\n",
        "        # line_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/' + str(line_pk) + '/'\n",
        "        # line_json={'region' : region_pk_list[region_nu]}\n",
        "        # res_line = requests.patch(line_url,headers=headers,data=json.dumps(line_json))\n",
        "        lines[n]['region']=region_pk_list[region_nu]\n",
        "        break\n",
        "      else:\n",
        "        region_nu+=1\n",
        "  print('putting')\n",
        "  bulk_json=json.dumps({'lines': lines})\n",
        "  #print(bulk_json[0:300])\n",
        "  bulk_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/bulk_update/'\n",
        "  #print(bulk_url)\n",
        "  res_bulk=requests.put(bulk_url,headers=headers,data=bulk_json)\n",
        "  print(res_bulk)\n",
        "  print('reordering')\n",
        "  reorder_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/recalculate_ordering/'\n",
        "  reorder_res = requests.post(reorder_url,headers=headers)\n",
        "\n",
        "\n",
        "def find_or_delete_unlinked_lines(doc_pk,part_pk,do_delete=False):\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  print(parts_url)\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  for line_n,line in enumerate(res['lines']):\n",
        "    if line['region']==None:\n",
        "      print('found unassociated line')\n",
        "      if do_delete:\n",
        "        delete_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/'+str(line['pk'])\n",
        "        r = requests.delete(delete_url,headers=headers)\n",
        "  if do_delete:\n",
        "    reorder_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/recalculate_ordering/'\n",
        "    reorder_res = requests.post(reorder_url,headers=headers)\n",
        "\n",
        "def delete_1p_lines(doc_pk,part_pk):\n",
        "\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  print(parts_url)\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  #print(res)\n",
        "\n",
        "  for line_n,line in enumerate(res['lines']):\n",
        "    if (len(line['baseline'])<2):\n",
        "      print('need to delete line')\n",
        "      delete_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/'+str(line['pk'])\n",
        "      r = requests.delete(delete_url,headers=headers)\n",
        "  reorder_url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/recalculate_ordering/'\n",
        "  reorder_res = requests.post(reorder_url,headers=headers)\n",
        "\n",
        "def find_lines_without_mask(doc_pk,part_pk):\n",
        "\n",
        "  from requests.compat import urljoin\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  for line_n,line in enumerate(res['lines']):\n",
        "    try:\n",
        "      len_mask=len(line['mask'])\n",
        "    except:\n",
        "      len_mask=0\n",
        "    if len_mask<4:\n",
        "      print(line_n,len_mask)\n",
        "\n",
        "def extend_lines(doc_pk,part_pk,extension=15,left_also=False,baseline2topline=False,repoly=True):\n",
        "\n",
        "  parts_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/')\n",
        "  res = requests.get(parts_url,headers=headers).json()\n",
        "  pagewidth=res['image']['size'][0]\n",
        "  for line in res['lines']:\n",
        "    baseline=line['baseline']\n",
        "    line_id=line['pk']\n",
        "    if (line['document_part']==part_pk):\n",
        "      if baseline[0][0]<baseline[-1][0]:\n",
        "        if left_also:\n",
        "          baseline[0][0]=max(5,int(round((baseline[0][0])))-extension)\n",
        "        baseline[-1][0]=min(int(round((baseline[-1][0])))+extension,pagewidth-5)\n",
        "      else:\n",
        "        if left_also:\n",
        "          baseline[0][0]=min(int(round((baseline[0][0])))+extension,pagewidth-5)\n",
        "        baseline[-1][0]=max(5,int(round((baseline[-1][0])))-extension)\n",
        "      if baseline2topline:\n",
        "        baseline=[[pt[0], max(5,pt[1]-y_decrease)] for pt in baseline]\n",
        "      line_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/' + str(line_id) + '/'\n",
        "      line_json = requests.get(line_url,headers=headers).json()\n",
        "      line_json['baseline']=baseline\n",
        "      res_line = requests.put(line_url,headers=headers,data=json.dumps(line_json))\n",
        "  if repoly:\n",
        "    url_repoly=urljoin(parts_url,'reset_masks/')\n",
        "    repoly_res=requests.post(url_repoly,headers=headers)\n",
        "\n",
        "def create_transcription_table(doc_pk,part_list,tr_level):\n",
        "\n",
        "  pages=[]\n",
        "  regions=[]\n",
        "  line_pks=[]\n",
        "  transcription_pks=[]\n",
        "  linenumbers=[]\n",
        "  txt=[]\n",
        "  regions=[]\n",
        "  for n,part_pk in enumerate(part_list):\n",
        "\n",
        "    if n % 10 == 0:\n",
        "      print(n)\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/transcriptions/?transcription='+str(tr_level)\n",
        "    res_transcriptions_this_part = requests.get(url,headers=headers).json()\n",
        "    line_pk_list_this_transcription = [tr.get('line') for tr in res_transcriptions_this_part['results']]\n",
        "\n",
        "    url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)\n",
        "    res_part = requests.get(url,headers=headers).json()\n",
        "    line_pk_list_this_part = [line.get('pk') for line in res_part['lines']]\n",
        "    line2region_list_this_part = [line.get('region') for line in res_part['lines']]\n",
        "\n",
        "    region_pk_list_this_part = [region.get('pk') for region in res_part['regions']]\n",
        "\n",
        "    for line in res_part['lines']:\n",
        "      pages.append(n+1)\n",
        "      line_pks.append(line['pk'])\n",
        "      linenumbers.append(line['order']+1)\n",
        "      try:\n",
        "        regions.append(region_pk_list_this_part.index(line['region'])+1)\n",
        "      except:\n",
        "        regions.append(None)\n",
        "      try:\n",
        "        index=line_pk_list_this_transcription.index(line['pk'])\n",
        "        transcription_pks.append(res_transcriptions_this_part['results'][index]['pk'])\n",
        "        txt.append(res_transcriptions_this_part['results'][index]['content'])\n",
        "      except:\n",
        "        transcription_pks.append(None)\n",
        "        txt.append('')\n",
        "  print('done')\n",
        "\n",
        "  columntitles = ['page', 'region', 'line','line_pk', 'transcription_pk','txt']\n",
        "  data =[columntitles] + list(zip(pages, regions, linenumbers,line_pks,transcription_pks,txt))\n",
        "\n",
        "  for i, d in enumerate(data):\n",
        "    tableline = ' | '.join(str(x).ljust(4) for x in d)\n",
        "    print(tableline)\n",
        "    if i == 0:\n",
        "      print('-' * len(tableline))\n",
        "\n",
        "def delete_empty_lines(doc_pk,part_pk,tr_level,min_length=1):\n",
        "\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_nu)+'/transcriptions/?transcription='+str(tr_level)\n",
        "  print(root_url+'document/'+str(doc_pk)+'/part/'+str(part_nu)+'/edit#')\n",
        "  res_transcriptions_this_part = requests.get(url,headers=headers).json()\n",
        "  line_pk_list_this_transcription = [tr.get('line') for tr in res_transcriptions_this_part['results']]\n",
        "\n",
        "  url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_nu)\n",
        "  res_part = requests.get(url,headers=headers).json()\n",
        "  line_pk_list_this_part = [line.get('pk') for line in res_part['lines']]\n",
        "  line2region_list_this_part = [line.get('region') for line in res_part['lines']]\n",
        "  region_pk_list_this_part = [region.get('pk') for region in res_part['regions']]\n",
        "\n",
        "  for line in res_part['lines']:\n",
        "    try:\n",
        "      index=line_pk_list_this_transcription.index(line['pk'])\n",
        "      txt=res_transcriptions_this_part['results'][index]['content']\n",
        "      if (txt=='') or (txt==' ') or len(txt)<min_length:\n",
        "        delete_url=url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_nu)+'/lines/'+str(line['pk'])\n",
        "        del_res=requests.delete(delete_url, headers=headers)\n",
        "        print(del_res.status_code)\n",
        "        print(delete_url)\n",
        "    except:\n",
        "      delete_url=url=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_nu)+'/lines/'+str(line['pk'])\n",
        "      del_res=requests.delete(delete_url, headers=headers)\n",
        "      print(del_res.status_code)\n",
        "\n",
        "def calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth):\n",
        "  average_line_height_list=[]\n",
        "  midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
        "  lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
        "  lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
        "  d_list=[]\n",
        "  for n,midpoint in enumerate(midpointList):\n",
        "    if n>0:\n",
        "      p1=array(lineBegList[n-1])\n",
        "      p2=array(lineEndList[n-1])\n",
        "      p3=array(midpoint)\n",
        "      d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
        "    elif len(midpointList)>1:\n",
        "      p1=array(lineBegList[n+1])\n",
        "      p2=array(lineEndList[n+1])\n",
        "      p3=array(midpoint)\n",
        "      d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
        "    average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
        "    average_line_height_list.append(average_line_height)\n",
        "  if average_line_height_list:\n",
        "    return int(round(mean(average_line_height_list)))\n",
        "  else:\n",
        "    return -1\n",
        "\n",
        "def restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,average_line_distance=0):\n",
        "\n",
        "  def repoly_extreme_line(line,direction):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    # create dummy baseline above or below to limit extension of first or last line\n",
        "    phantombaseline=[[pt[0], min(pageheight-5,max(5,pt[1]+(average_line_distance+5)*direction))] for pt in line[2]]\n",
        "    phantom_data=json.dumps({'document_part':part_pk,'region':region_pk,'baseline':phantombaseline})\n",
        "    phantom_line_pk=requests.post(lines_url,headers=headers,data=phantom_data).json()['pk']\n",
        "    # repolygonize\n",
        "    res=repolygonize_line(doc_pk,part_pk,line[0])\n",
        "    print('repoly-response: ',res)\n",
        "    # delete dummy line\n",
        "    phantom_line_del=requests.delete(lines_url+str(phantom_line_pk),headers=headers)\n",
        "  print(average_line_distance)\n",
        "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
        "  all_lines_this_region=get_all_lines_of_region(doc_pk,part_pk,region_pk)\n",
        "# main lines only\n",
        "  main_lines_this_region =[line for line in all_lines_this_region if line['typology'==None]]\n",
        "  baselineCoordsList=[line[2] for line in main_lines_this_region]\n",
        "# calculate average line distance\n",
        "  if average_line_distance==0:\n",
        "    average_line_distance=calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth)\n",
        "  if (average_line_distance>5) and (len(main_lines_this_region)>0):\n",
        "# treat first line\n",
        "    index_first = min(lines[1] for lines in main_lines_this_region)\n",
        "    line_first=[line for line in main_lines_this_region if line[1]==index_first]\n",
        "    line_first=line_first[0]\n",
        "    repoly_extreme_line(line_first,-1)\n",
        "# treat last line\n",
        "    index_last = max(lines[1] for lines in main_lines_this_region)\n",
        "    if not(index_last==index_first):\n",
        "      line_last=[line for line in main_lines_this_region if line[1]==index_last]\n",
        "      line_last=line_last[0]\n",
        "      repoly_extreme_line(line_last,1)\n",
        "      print('repolygonized extremes of part: ',part_pk)\n",
        "  else:\n",
        "    print('not able to calculate average line distance for part: ',part_pk)\n",
        "  return average_line_distance\n",
        "\n",
        "def create_normalized_polygons_around_CORRECTION_lines(doc_pk,part_pk,ascender=12,descender=24,correctionlinetype=3,safetydistance=5):\n",
        "\n",
        "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
        "  ## first loop: calculate average line height and distance\n",
        "  region_list=get_main_region_pk_list(doc_pk,part_pk)\n",
        "  for m,region_pk in enumerate(region_list):\n",
        "    all_lines_this_region=get_all_lines_of_region(doc_pk,part_pk,region_pk)\n",
        "    correction_lines_this_region = [line for line in all_lines_this_region if line[3]==correctionlinetype]\n",
        "    baselineCoordsList = [line[2] for line in correction_lines_this_region]\n",
        "    baselinePKList = [line[0] for line in correction_lines_this_region]\n",
        "    for n,line in enumerate(baselineCoordsList):\n",
        "      old_pt=line[0]\n",
        "      for o,pt in enumerate(line[1:]):\n",
        "        #print('region '+str(m)+', line '+str(n)+', point '+str(o)+str(old_pt)+':'+str(pt))\n",
        "        angle=get_angle([old_pt,pt])\n",
        "        if o==0:\n",
        "          inside_above=False\n",
        "          inside_below=False\n",
        "          new_pt_above=rotateNP((old_pt[0],old_pt[1]-ascender),origin=tuple(old_pt),degrees=angle)\n",
        "          new_pt_below=rotateNP((old_pt[0],old_pt[1]+descender),origin=tuple(old_pt),degrees=angle)\n",
        "          #print(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "          new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "          new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
        "          #print(new_pt_above)\n",
        "          polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
        "          polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
        "        new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-ascender),origin=tuple(pt),degrees=angle)]\n",
        "        new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+descender),origin=tuple(pt),degrees=angle)]\n",
        "        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
        "        if o>0:\n",
        "          if inside_above and not(o==len(line[1:])): # if point is inside and is not the last point, # delete previous point, dont insert next point either # and insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
        "            replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
        "            del polygon_above[-2:-1]\n",
        "            polygon_above.append(replace_pt)\n",
        "          if inside_below:\n",
        "            replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
        "            del polygon_below[0:1]\n",
        "            polygon_below.insert(0,replace_pt)\n",
        "          pol=Polygon(polygon_above+polygon_below)\n",
        "          inside_above=pol.contains(Point(new_pt_above))\n",
        "          inside_below=pol.contains(Point(new_pt_below))\n",
        "        polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
        "        polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
        "        old_pt=pt\n",
        "      if inside_above:\n",
        "        del polygon_above[-1] # if point is inside and is last point delete it\n",
        "      if inside_below:\n",
        "        del polygon_below[0] # if point is inside and is last point delete it\n",
        "      polygon_boundary=polygon_above+polygon_below\n",
        "      line_url = urljoin(root_url,'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'+str(baselinePKList[n])+'/')\n",
        "      resline = requests.get(line_url,headers=headers).json()\n",
        "      resline['mask']=polygon_boundary\n",
        "      data=json.dumps(resline)\n",
        "      rput = requests.put(line_url,headers=headers,data=data)\n",
        "      print(m,rput)\n",
        "\n",
        "\n",
        "def export_txt_file_of_main_text_in_main_regions(doc_nu,part_list,tr_level_pk,dirname,fname):\n",
        "\n",
        "    #os.chdir(dirname)\n",
        "    #file = open(fname, 'w+',encoding=\"utf8\")\n",
        "    resultstr=''\n",
        "    for n,part_pk in enumerate(part_list):\n",
        "        #print(n,part_pk)\n",
        "        #get all main regions\n",
        "        page_nu=0\n",
        "        main_region_list=[]\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            regions_url = root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/blocks/'+'?page='+str(page_nu)\n",
        "         #   print(regions_url)\n",
        "            regions=requests.get(regions_url,headers=headers).json()\n",
        "            main_region_list=main_region_list+[r.get('pk') for r in regions['results'] if r['typology']==2]\n",
        "            if not(regions['next']):\n",
        "                break\n",
        "        #get all transcriptions of this part\n",
        "        transcriptions={}\n",
        "        page_nu=0\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(tr_level_pk)\n",
        "            res = requests.get(tr_url, headers=headers).json()\n",
        "            for tr_line in enumerate(res['results']):\n",
        "                transcriptions[tr_line[1]['line']]=tr_line[1]['content']\n",
        "            if not(res['next']):\n",
        "                break\n",
        "\n",
        "        #get all lines of this part that are in the main columns and of the default type\n",
        "        page_nu=0\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            lines_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/lines/'+'?page='+str(page_nu)\n",
        "            res = requests.get(lines_url, headers=headers).json()\n",
        "            for m,line in enumerate(res['results']):\n",
        "                if (line['typology']==None) and (line['region'] in main_region_list):\n",
        "                    try:\n",
        "                        content=transcriptions[line['pk']]\n",
        "                    except:\n",
        "                        content=''\n",
        "                    table_line=str(n+1)+'\\t'+str(part_pk)+'\\t'+str(line['order'])+'\\t'+str(line['pk'])+'\\t'+content+'\\n'\n",
        "                    resultstr=resultstr+table_line\n",
        "     #               file.write(table_line)\n",
        "                    print(table_line)\n",
        "            if not(res['next']):\n",
        "                break\n",
        "    #file.close()\n",
        "    return resultstr\n",
        "    print('done')\n",
        "\n",
        "\n",
        "\n",
        "def simplify_hebrew(doc_nu,part_list,source_tr_level,target_tr_level):\n",
        "    vocalization=[1425,1427,1430,1436,1438,1443,1446,1453,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1471,1473,1474,1476,1477,1478,1479]\n",
        "    singlequot=[1523,8216,8217,8219,8242]\n",
        "    doublequot=[1524,8220,8221,8222,8223,8243]\n",
        "    spacechars=[9,160,8201,8195,8192]\n",
        "    deletechars=[1565,8299,8205,8300,8302]\n",
        "    toreplacechars=[(64296,1514),(64298,1513),(64299,1513),(64300,1513),(64302,1488),(64303,1488),(64305,1489),(64306,1490),(64307,1491),(64309,1493),(64315,1499),(64324,1508),(64327,1511),(64330,1514),(64331,1493),(64332,1489),(64333,1499),(64334,1508),(8277,42),(8283,46),(11799,61)]\n",
        "    for n,part in enumerate(part_list[41:]):\n",
        "        target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'\n",
        "        #https://www.escriptorium.fr/api/documents/39/parts/14480/transcriptions/?page=2&transcription=46\n",
        "        page_nu=0\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            source_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(source_tr_level)\n",
        "            target_tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(target_tr_level)\n",
        "            res = requests.get(source_tr_url, headers=headers).json()\n",
        "            print(source_tr_url)\n",
        "            res_target=requests.get(target_tr_url, headers=headers).json()\n",
        "            if res_target!={'detail': 'Invalid page.'}:\n",
        "                target_line_pk_list = [line['line'] for line in res_target['results']]\n",
        "                target_tr_pk_list = [line['pk'] for line in res_target['results']]\n",
        "                print(target_line_pk_list)\n",
        "            else:\n",
        "                target_line_pk_list=[]\n",
        "                target_tr_pk_list=[]\n",
        "            #print(res_target)\n",
        "            for m,line in enumerate(res['results']):\n",
        "                #'line': 674381, 'transcription': 46, 'content': '    ',\n",
        "                line_pk=line['line']\n",
        "                print(line_pk)\n",
        "                print(line['pk'])\n",
        "                content=line['content']\n",
        "                for u in vocalization:\n",
        "                    content=content.replace(chr(u),'')\n",
        "                for u in spacechars:\n",
        "                    content=content.replace(chr(u),' ')\n",
        "                for u in deletechars:\n",
        "                    content=content.replace(chr(u),'')\n",
        "                for u in singlequot:\n",
        "                    content=content.replace(chr(u),\"'\")\n",
        "                for u in doublequot:\n",
        "                    content=content.replace(chr(u),'\"')\n",
        "                for (x,y) in toreplacechars:\n",
        "                    content=content.replace(chr(x),chr(y))\n",
        "                data = {'line': line_pk, 'transcription': target_tr_level, 'content': content}\n",
        "                if line_pk in target_line_pk_list:\n",
        "                    t_url=(root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/transcriptions/'+str(line['pk'])+'/')\n",
        "                    res_t=requests.get(t_url,headers=headers).json()\n",
        "                    print('put',res_t['content'])\n",
        "                    res_t['content']=content\n",
        "                    print(t_url)\n",
        "                    data=json.dumps(res_t, ensure_ascii=False).encode('utf8')\n",
        "                    res_put = requests.put(t_url, data=data, headers=headers)\n",
        "                    #print('put',res_put.content)\n",
        "                else:\n",
        "                    res_post = requests.post(target_tr_url, data=json.dumps(data), headers=headers)\n",
        "                    #print(n,part,m,res_post.content)\n",
        "            if not(res['next']):\n",
        "                break\n",
        "    print('done simplifying hebrew')\n",
        "\n",
        "def delete_text_in_main_regions_except_line_type(doc_nu,part_list,tr_level,allowed_line_types):\n",
        "\n",
        "    for n,part_pk in enumerate(part_list):\n",
        "        print(n,part_pk)\n",
        "        #get all main regions\n",
        "        regions_url = root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/blocks/'\n",
        "        regions=requests.get(regions_url,headers=headers).json()\n",
        "        main_region_list=[r.get('pk') for r in regions['results'] if r['typology']==2]\n",
        "\n",
        "        #get all lines of this part that are in the main columns and of the default type and delete their content\n",
        "        page_nu=0\n",
        "        del_lines_pk_list=[]\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            lines_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/lines/'+'?page='+str(page_nu)\n",
        "            res = requests.get(lines_url, headers=headers).json()\n",
        "            for line in res['results']:\n",
        "                if (not(line['typology'] in allowed_line_types)) and (line['region'] in main_region_list):\n",
        "                    del_lines_pk_list.append(line['pk'])\n",
        "            if not(res['next']):\n",
        "                break\n",
        "\n",
        "        page_nu=0\n",
        "        while True:\n",
        "            page_nu+=1\n",
        "            tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/transcriptions/'+'?page='+str(page_nu)+'&transcription='+str(tr_level)\n",
        "            res = requests.get(tr_url, headers=headers).json()\n",
        "            print(del_lines_pk_list)\n",
        "            for tr_line in res['results']:\n",
        "                if (tr_line['line'] in del_lines_pk_list) and not(tr_line['content']==''):\n",
        "                    tr_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part_pk)+'/transcriptions/'+str(tr_line['pk'])+'/'\n",
        "                    print(tr_url)\n",
        "                    data = {'line': tr_line['line'], 'transcription': tr_level, 'content': ''}\n",
        "                    resput = requests.put(tr_url, data=json.dumps(data), headers=headers)\n",
        "                    print(resput)\n",
        "            if not(res['next']):\n",
        "                break\n",
        "    print('done')\n",
        "\n",
        "def restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,average_line_distance=0):\n",
        "\n",
        "  def repoly_extreme_line(line,direction):\n",
        "    lines_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/lines/'\n",
        "    # create dummy baseline above or below to limit extension of first or last line\n",
        "    #single line: direction=0\n",
        "    phantom_line_pks=[]\n",
        "    if not(direction==0): #NOT a single-line-region (direction=0) but a top or bottom line of multiline region\n",
        "      phantombaseline=[[pt[0], min(pageheight-5,max(5,pt[1]+(average_line_distance+5)*direction))] for pt in line[2]]\n",
        "      phantom_data=json.dumps({'document_part':part_pk,'region':region_pk,'baseline':phantombaseline})\n",
        "      phantom_line_pks.append(requests.post(lines_url,headers=headers,data=phantom_data).json()['pk'])\n",
        "    else: # if region consists of single line create dummy lines above AND below\n",
        "      for direction in [-1,1]:\n",
        "        phantombaseline=[[pt[0], min(pageheight-5,max(5,pt[1]+(average_line_distance+5)*direction))] for pt in line[2]]\n",
        "        phantom_data=json.dumps({'document_part':part_pk,'region':region_pk,'baseline':phantombaseline})\n",
        "        phantom_line_pks.append(requests.post(lines_url,headers=headers,data=phantom_data).json()['pk'])\n",
        "    # repolygonize\n",
        "    res=repolygonize_line(doc_pk,part_pk,line[0])\n",
        "    print('repoly-response: ',res)\n",
        "    sleep(2)\n",
        "    # delete dummy line(s)\n",
        "    for phantom_line_pk in phantom_line_pks:\n",
        "      phantom_line_del=requests.delete(lines_url+str(phantom_line_pk),headers=headers)\n",
        "  print(average_line_distance)\n",
        "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
        "  all_lines_this_region=get_all_lines_of_region(doc_pk,part_pk,region_pk)\n",
        "# main lines only\n",
        "  main_lines_this_region =[line for line in all_lines_this_region if line['typology'==None]]\n",
        "  baselineCoordsList=[line[2] for line in main_lines_this_region]\n",
        "# calculate average line distance\n",
        "  if average_line_distance==0:\n",
        "    average_line_distance=calculate_average_line_distance(baselineCoordsList,pageheight,pagewidth)\n",
        "  if (average_line_distance>5) and (len(main_lines_this_region)>0):\n",
        "# treat first line\n",
        "    index_first = min(lines[1] for lines in main_lines_this_region)\n",
        "    line_first=[line for line in main_lines_this_region if line[1]==index_first]\n",
        "    line_first=line_first[0]\n",
        "    index_last = max(lines[1] for lines in main_lines_this_region)\n",
        "# treat last line\n",
        "    if not(index_last==index_first):\n",
        "      repoly_extreme_line(line_first,-1)\n",
        "      line_last=[line for line in main_lines_this_region if line[1]==index_last]\n",
        "      line_last=line_last[0]\n",
        "      repoly_extreme_line(line_last,1)\n",
        "    else:\n",
        "      repoly_extreme_line(line_first,0) #direction 0 == single line region\n",
        "    print('repolygonized extremes of part: ',part_pk)\n",
        "  else:\n",
        "    print('not able to calculate average line distance for part: ',part_pk)\n",
        "  return average_line_distance\n",
        "\n",
        "\n",
        "def loop_document_restrict_extreme_lines_all_regions(doc_pk,fix_line_height,start_item=0):\n",
        "\n",
        "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
        "  for part_pk in part_pk_list[start_item:]:\n",
        "    print('----------------------------------------------------')\n",
        "    print('resegmenting part :',part_pk)\n",
        "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
        "    main_region_list=get_main_region_pk_list(doc_pk,part_pk)\n",
        "    average_line_height_in_main_regions=[]\n",
        "    for region_pk in main_region_list:\n",
        "      average_line_height_in_main_regions.append(restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,fix_line_height))\n",
        "    if len(main_region_list)>0:\n",
        "      global_average_line_height=mean(average_line_height_in_main_regions)\n",
        "    else:\n",
        "      global_average_line_height=fix_line_height\n",
        "    non_main_regions=set(region_list)-set(main_region_list)\n",
        "    for region_pk in non_main_regions:\n",
        "      line_height=restrict_first_and_last_line_polygon_according_to_average_line_height(doc_pk,part_pk,region_pk,global_average_line_height)\n",
        "\n",
        "  print('done')\n",
        "\n",
        "def create_normalized_polygons_around_lines(doc_nu,splitfactor=0.6,safetydistance=5,basic_line_height=120):\n",
        "\n",
        "    part_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_nu)\n",
        "\n",
        "    for n,part in enumerate(part_list):\n",
        "        parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
        "        res = requests.get(parts_url,headers=headers).json()\n",
        "        pagewidth=res['image']['size'][0]\n",
        "        pageheight=res['image']['size'][1]\n",
        "        print(parts_url)\n",
        "        ## first loop: calculate average line height and distance\n",
        "        regionList = [region for region in res['regions']]\n",
        "        average_line_height_list=[]\n",
        "        average_line_height_not_null=[]\n",
        "        for m,region in enumerate(regionList):\n",
        "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
        "            midpointList = [[(line[0][0]+line[-1][0])/2,(line[0][1]+line[-1][1])/2] for line in baselineCoordsList]\n",
        "            lineBegList = [line_intersection([[0,0],[0,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
        "            lineEndList = [line_intersection([[pagewidth,0],[pagewidth,pageheight]],[[line[0][0],line[0][1]],[line[-1][0],line[-1][1]]]) for line in baselineCoordsList]\n",
        "            d_list=[]\n",
        "            for n,midpoint in enumerate(midpointList):\n",
        "                if n>0:\n",
        "                    p1=array(lineBegList[n-1])\n",
        "                    p2=array(lineEndList[n-1])\n",
        "                    p3=array(midpoint)\n",
        "                    d_list.append(cross(p2-p1,p3-p1)/linalg.norm(p2-p1))\n",
        "                elif len(midpointList)>1:\n",
        "                    p1=array(lineBegList[n+1])\n",
        "                    p2=array(lineEndList[n+1])\n",
        "                    p3=array(midpoint)\n",
        "                    d_list.append(-(cross(p2-p1,p3-p1)/linalg.norm(p2-p1)))\n",
        "            if not(d_list):\n",
        "                print('one line region')\n",
        "                average_line_height_list.append(0)\n",
        "            else:\n",
        "                average_line_height=int(round(sum(d_list)/len(d_list),0))\n",
        "                average_line_height_list.append(average_line_height)\n",
        "                average_line_height_not_null.append(average_line_height)\n",
        "        print(average_line_height_list)\n",
        "        print(average_line_height_not_null)\n",
        "        if not(average_line_height_not_null):\n",
        "            global_average_line_height=basic_line_height\n",
        "            print('bad average line height')\n",
        "        else:\n",
        "            global_average_line_height=int(round(sum(average_line_height_not_null)/len(average_line_height_not_null),0))\n",
        "            print(global_average_line_height)\n",
        "\n",
        "        for m,region in enumerate(regionList):\n",
        "            if average_line_height_list[m]==0:\n",
        "                this_line_height=global_average_line_height\n",
        "            else:\n",
        "                this_line_height=average_line_height_list[m]\n",
        "            baselineCoordsList = [line['baseline'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
        "            baselinePKList = [line['pk'] for line in res['lines'] if line['region']==regionList[m]['pk']]\n",
        "            for n,line in enumerate(baselineCoordsList):\n",
        "                old_pt=line[0]\n",
        "                for o,pt in enumerate(line[1:]):\n",
        "                    #print('region '+str(m)+', line '+str(n)+', point '+str(o)+str(old_pt)+':'+str(pt))\n",
        "                    angle=get_angle([old_pt,pt])\n",
        "                    if o==0:\n",
        "                        inside_above=False\n",
        "                        inside_below=False\n",
        "                        new_pt_above=rotateNP((old_pt[0],old_pt[1]-this_line_height*splitfactor),origin=tuple(old_pt),degrees=angle)\n",
        "                        new_pt_below=rotateNP((old_pt[0],old_pt[1]+this_line_height*(1-splitfactor)),origin=tuple(old_pt),degrees=angle)\n",
        "                        #print(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "                        new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "                        new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
        "                        #print(new_pt_above)\n",
        "                        polygon_above=[[int(coordinate) for coordinate in new_pt_above]]\n",
        "                        polygon_below=[[int(coordinate) for coordinate in new_pt_below]]\n",
        "                    new_pt_above=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]-this_line_height*splitfactor),origin=tuple(pt),degrees=angle)]\n",
        "                    new_pt_below=[int(coordinate) for coordinate in rotateNP((pt[0],pt[1]+this_line_height*(1-splitfactor)),origin=tuple(pt),degrees=angle)]\n",
        "                    new_pt_above=check_point_in_image(new_pt_above,pageheight,pagewidth,safetydistance)\n",
        "                    new_pt_below=check_point_in_image(new_pt_below,pageheight,pagewidth,safetydistance)\n",
        "                    if o>0:\n",
        "                        if inside_above and not(o==len(line[1:])): # if point is inside and is not the last point, # delete previous point, dont insert next point either # and insert instead the intersection of (p-2:p-1) and (p:p+1)\n",
        "                            replace_pt=list(line_intersection([polygon_above[-3],polygon_above[-2]],[polygon_above[-1],new_pt_above]))\n",
        "                            del polygon_above[-2:-1]\n",
        "                            polygon_above.append(replace_pt)\n",
        "                        if inside_below:\n",
        "                            replace_pt=list(line_intersection([polygon_below[2],polygon_below[1]],[polygon_below[0],new_pt_below]))\n",
        "                            del polygon_below[0:1]\n",
        "                            polygon_below.insert(0,replace_pt)\n",
        "                        pol=Polygon(polygon_above+polygon_below)\n",
        "                        inside_above=pol.contains(Point(new_pt_above))\n",
        "                        inside_below=pol.contains(Point(new_pt_below))\n",
        "                    polygon_above.append(new_pt_above) # if point is outside add it to boundary\n",
        "                    polygon_below.insert(0,new_pt_below) # if point is outside add it to boundary\n",
        "                    old_pt=pt\n",
        "                if inside_above:\n",
        "                    del polygon_above[-1] # if point is inside and is last point delete it\n",
        "                if inside_below:\n",
        "                    del polygon_below[0] # if point is inside and is last point delete it\n",
        "                polygon_boundary=polygon_above+polygon_below\n",
        "                line_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/lines/'+str(baselinePKList[n])+'/')\n",
        "                resline = requests.get(line_url,headers=headers).json()\n",
        "                resline['mask']=polygon_boundary\n",
        "                data=json.dumps(resline)\n",
        "                rput = requests.put(line_url,headers=headers,data=data)\n",
        "                print(m,rput)\n",
        "    print('done')\n",
        "\n",
        "def merge2levels_in_third(doc_pk,part_pk,level1,level2,level3):\n",
        "  lines_without_master_transcription,proportion,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level1)\n",
        "  if proportion>0.3:\n",
        "    lines_without_level2_transcription,proportion_level2,empty_lines=get_lines_without_transcription_level(doc_pk,part_pk,level2)\n",
        "    if proportion_verso<0.3:\n",
        "      print('from level2 to master on part ',part_pk)\n",
        "      transfer_txt2level(doc_pk,part_pk,level2,level3)\n",
        "    else:\n",
        "      print('from level1 to master on part ',part_pk)\n",
        "      transfer_txt2level(doc_pk,part_pk,level1,level3)\n",
        "  else:\n",
        "    print('master exists for part ',part_pk)\n",
        "\n",
        "def change_all_regions2main(dok_pk):\n",
        "  part_pk_list,tr_level_list,region_type_list,line_type_list=get_basic_info(doc_pk)\n",
        "  for n,part_pk in enumerate(part_pk_list):\n",
        "    region_list=get_region_pk_list(doc_pk,part_pk)\n",
        "    for region_pk in region_list:\n",
        "      regions_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/blocks/'+str(region_pk)+'/'\n",
        "      res=requests.get(regions_url,headers=headers).json()\n",
        "      print(n,res['typology'])\n",
        "      if not(res['typology']==2):\n",
        "        res['typology']=2\n",
        "        data=json.dumps(res)\n",
        "        res2=requests.put(regions_url,headers=headers,data=data)\n",
        "        print(regions_url)\n",
        "        print(res2)\n",
        "\n",
        "def plot_polygon(Polygon):\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  plt.plot(*Polygon.exterior.xy)\n",
        "  #plt.plot(*maskSimple.exterior.xy)\n",
        "\n",
        "def move_line_up_down(baseline,distance,down,pageheight,pagewidth):\n",
        "  #down=1\n",
        "  #up=-1\n",
        "  safety=5\n",
        "  angle=get_angle(baseline)\n",
        "  # 0=horizontal\n",
        "  if abs(angle)<45:\n",
        "    if baseline[0][0]<baseline[-1][0]:\n",
        "      vectorx,vectory=get_vector(angle+90*down,distance) #(LTR)\n",
        "    else:\n",
        "      vectorx,vectory=get_vector(angle+90*down,distance) #(RTL) (upside down)\n",
        "  else:\n",
        "    if baseline[0][1]<baseline[-1][1]:\n",
        "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical up)\n",
        "    else:\n",
        "      vectorx,vectory=get_vector(angle+90*down,distance) #(vertical down)\n",
        "  new_baseline=[[max(safety,min(pagewidth-safety,p[0]+vectorx)),max(safety,min(pageheight-safety,p[1]+vectory))] for p in baseline]\n",
        "  return(new_baseline)\n",
        "\n",
        "def extend_single_line(baseline,distance,do_left,do_right,pageheight,pagewidth):\n",
        "  angle=get_angle(baseline)\n",
        "  # 0=horizontal\n",
        "  vectorx,vectory=get_vector(angle,distance)\n",
        "  p1=baseline[0]\n",
        "  pz=baseline[-1]\n",
        "  p0=[]\n",
        "  pend=[]\n",
        "  if abs(angle)<45:\n",
        "    if baseline[0][0]<baseline[-1][0]:\n",
        "      if do_left:\n",
        "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
        "      if do_right:\n",
        "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
        "    else:\n",
        "      if do_left:\n",
        "        p0=[[max(1,min(pagewidth,p1[0]+vectorx)),max(1,min(pageheight,p1[1]+vectory))]]\n",
        "      if do_right:\n",
        "        pend=[[max(1,min(pagewidth,pz[0]-vectorx)),max(1,min(pageheight,pz[1]-vectory))]]\n",
        "  else:\n",
        "    if baseline[0][1]<baseline[-1][1]:\n",
        "      if do_left:\n",
        "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
        "      if do_right:\n",
        "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
        "    else:\n",
        "      if do_left:\n",
        "        p0=[[max(1,min(pagewidth,p1[0]-vectorx)),max(1,min(pageheight,p1[1]-vectory))]]\n",
        "      if do_right:\n",
        "        pend=[[max(1,min(pagewidth,pz[0]+vectorx)),max(1,min(pageheight,pz[1]+vectory))]]\n",
        "  if not(p0==[]):\n",
        "    baseline=baseline[1:]\n",
        "  if not(pend==[]):\n",
        "    baseline=baseline[:-1]\n",
        "  new_baseline=p0+baseline+pend\n",
        "  return(new_baseline)\n",
        "\n",
        "\n",
        "def move_all_lines_of_part_up_down(doc_pk,part_pk,distance=10,down=1,repoly=True, output=False):\n",
        "  line_list=get_all_full_lines_of_part(doc_pk,part_pk)\n",
        "  #[[line['pk'],line['order'],line['baseline'],line['typology'],line['region']] for line in data['results']]\n",
        "  pageheight,pagewidth=page_height_width(doc_pk,part_pk)\n",
        "  print(part_pk)\n",
        "  for line in line_list:\n",
        "    baseline=line['baseline']\n",
        "    line_id=line['pk']\n",
        "    new_baseline=move_line_up_down(baseline,distance,down,pageheight,pagewidth)\n",
        "    if output:\n",
        "      #print('angle:',int(round(angle,0)))\n",
        "      print('old: ',baseline)\n",
        "      print('new: ',new_baseline)\n",
        "    line_url = root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+ '/lines/' + str(line_id) + '/'\n",
        "    line_json = requests.get(line_url,headers=headers).json()\n",
        "    line_json['baseline']=new_baseline\n",
        "    res_line = requests.put(line_url,headers=headers,data=json.dumps(line_json))\n",
        "  if repoly:\n",
        "    url_repoly=root_url+'api/documents/'+str(doc_pk)+'/parts/'+str(part_pk)+'/reset_masks/'\n",
        "    repoly_res=requests.post(url_repoly,headers=headers)\n",
        "\n",
        "def calculate_regions_for_unlinked_lines(doc_nu,part,margin_type,paratext_type):\n",
        "# calculate regions for unlinked lines by taking the minima and maxima of their boundaries\n",
        "    from shapely.ops import cascaded_union\n",
        "\n",
        "\n",
        "    parts_url = urljoin(root_url,'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/')\n",
        "    print(parts_url)\n",
        "    res = requests.get(parts_url,headers=headers).json()\n",
        "    regpoly_list=[]\n",
        "    line_links_list=[]\n",
        "\n",
        "    for line_n,lines in enumerate(res['lines']):\n",
        "        if lines['region']==None:\n",
        "            line_array=array(lines['mask'])\n",
        "            try:\n",
        "                x1 = min(line_array[:,0])\n",
        "                y1 = min(line_array[:,1])\n",
        "                x2 = max(line_array[:,0])\n",
        "                y2 = max(line_array[:,1])\n",
        "                poly = Polygon([[x1,y1],[x2,y1],[x2,y2],[x1,y2]])\n",
        "                if len(regpoly_list)==0:\n",
        "                    regpoly_list.append(poly)\n",
        "                    line_links_list=[[line_n]]\n",
        "                else:\n",
        "                    no_join_found = True\n",
        "                    for n,poly1 in enumerate(regpoly_list):\n",
        "                        if poly.intersects(poly1): #poly overlaps with poly1 :\n",
        "                    # merge: https://deparkes.co.uk/2015/02/28/how-to-merge-polygons-in-python/\n",
        "                            polygons = [poly1, poly]\n",
        "                            union_poly = cascaded_union(polygons)\n",
        "                    # replace poly1 by union_poly\n",
        "                            regpoly_list[n] = union_poly\n",
        "                            poly = union_poly\n",
        "                            line_links_list[n].append(line_n)\n",
        "                            no_join_found = False\n",
        "                    if no_join_found:\n",
        "                        regpoly_list.append(poly)\n",
        "                        line_links_list.append([line_n])\n",
        "            except:\n",
        "                print('error at:',str(part))\n",
        "    blocks_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/'\n",
        "    data=requests.get(blocks_url,headers=headers).json()\n",
        "    existing_regions=data['regions']\n",
        "    post_url = root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'\n",
        "    max_region_n=-1\n",
        "    if len(existing_regions)==0:\n",
        "        max_region_size=0\n",
        "        for region_n,region in enumerate(regpoly_list):\n",
        "            this_region_area=region.area\n",
        "            if this_region_area>max_region_size:\n",
        "                max_region_size=this_region_area\n",
        "                max_region_n=region_n\n",
        "        maincol=regpoly_list[max_region_n]\n",
        "        maincol_point_list = [[int(float(j)) for j in i] for i in list(maincol.exterior.coords)]\n",
        "        block_json={'document_part': part,'box': maincol_point_list,'typology': 2}\n",
        "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
        "        y_main_min = min(array(maincol_point_list)[:,1])\n",
        "        y_main_max = max(array(maincol_point_list)[:,1])\n",
        "    else:\n",
        "        y_main_min=10000\n",
        "        y_main_max=0\n",
        "        for region in existing_regions:\n",
        "            if len(region['box'])<3:\n",
        "                print('delete '+str(region['pk']))\n",
        "                delete_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+ '/blocks/'+str(region['pk']) + '/'\n",
        "                r = requests.delete(delete_url,headers=headers)\n",
        "                print(r.status_code)\n",
        "            else:\n",
        "                maincol=Polygon(region['box'])\n",
        "                y1 = min(array(region['box'])[:,1])\n",
        "                if y1<y_main_min:\n",
        "                    y_main_min=y1\n",
        "                y2 = max(array(region['box'])[:,1])\n",
        "                if y2>y_main_max:\n",
        "                    y_main_max=y2\n",
        "    if max_region_n>-1:\n",
        "        del regpoly_list[max_region_n] # KICK MAINCOL out of REGPOLYLIST\n",
        "    for region_n,poly in enumerate(regpoly_list):\n",
        "        poly_point_list = [[int(float(j)) for j in i] for i in list(poly.exterior.coords)]\n",
        "        y_poly = mean(array(poly_point_list)[:,1])\n",
        "        if (y_poly<y_main_min) or (y_poly>y_main_max):\n",
        "            region_type=paratext_type\n",
        "        else:\n",
        "            region_type=margin_type\n",
        "        block_json={'document_part': part,'box': poly_point_list,'typology': region_type}\n",
        "        r = requests.post(post_url,headers=headers, data=json.dumps(block_json))\n",
        "    reorder_url=root_url+'api/documents/'+str(doc_nu)+'/parts/'+str(part)+'/recalculate_ordering/'\n",
        "    reorder_res = requests.post(reorder_url,headers=headers)\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        " # print(\"<> Running the module as a script! <>\")"
      ],
      "metadata": {
        "id": "elSj7n0r9B7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import functions\n",
        "from __init__ import serverconnections\n",
        "#from basic_api_functions import *\n",
        "#from complex_api_functions import *\n",
        "from numpy import rad2deg,deg2rad,array,atleast_2d,cos,sin,arctan2,squeeze,cross,linalg\n",
        "import numpy\n",
        "import math\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from requests.compat import urljoin\n",
        "from shapely.geometry import Point, Polygon\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import collections\n",
        "from google.colab import files\n",
        "!pip install bidict\n",
        "import bidict\n",
        "!pip install xlsxwriter\n",
        "import xlsxwriter"
      ],
      "metadata": {
        "id": "vtIA88-F9PsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}