{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Kraken_Makefile_Colab_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.018671"
      },
      "source": [
        "\n",
        "# ðŸ“’ Kraken Training on Colab â€” Makefile-Style (msia compatible)\n",
        "\n",
        "This notebook mirrors a **Makefile-style** workflow for training **Kraken** models:\n",
        "- Pull data from **Google Drive** and/or **local uploads**\n",
        "- Train **segmentation** and **recognition** with **custom learning rates**\n",
        "- Evaluate recognition (CER/WER)\n",
        "- Package outputs for download\n",
        "- (Optional) Upload `.mlmodel` directly to your eScriptorium instance (**msia.escriptorium.fr**)\n",
        "\n",
        "> In Colab: **Runtime â†’ Change runtime type â†’ GPU** before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018712"
      },
      "source": [
        "\n",
        "# 0) GPU check\n",
        "!nvidia-smi || true\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.01874"
      },
      "source": [
        "\n",
        "## Optional: Detect Kraken version from an existing model\n",
        "Upload **any working `.mlmodel`** from your instance to pin to a compatible line.  \n",
        "If detection fails or you skip, we fall back to a Colab-friendly default.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018765"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "up = files.upload()  # you may skip\n",
        "MODEL_PATH = next(iter(up)) if up else None\n",
        "MODEL_PATH\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018802"
      },
      "source": [
        "\n",
        "# Decide a kraken pin robustly for Colab (Python 3.12 compatible)\n",
        "import re, sys\n",
        "detected = None\n",
        "if MODEL_PATH:\n",
        "    try:\n",
        "        import h5py\n",
        "        with h5py.File(MODEL_PATH, \"r\") as h:\n",
        "            for key in (\"version\",\"kraken_version\",\"software\",\"generator\",\"creator\"):\n",
        "                if key in h.attrs:\n",
        "                    val = h.attrs[key]\n",
        "                    if isinstance(val, (bytes, bytearray)): val = val.decode()\n",
        "                    m = re.search(r\"(\\d+\\.\\d+(?:\\.\\d+)?)\", str(val))\n",
        "                    if m:\n",
        "                        detected = m.group(1); break\n",
        "    except Exception as e:\n",
        "        print(\"Model inspection skipped:\", e)\n",
        "\n",
        "if detected:\n",
        "    major, minor = detected.split('.')[:2]\n",
        "    KR_PIN = f\"kraken=={major}.{minor}.*\"\n",
        "else:\n",
        "    # Colab often runs Python 3.12; kraken 5.2.* won't install there.\n",
        "    KR_PIN = \"kraken==5.3.0\" if sys.version_info >= (3,12) else \"kraken==5.2.*\"\n",
        "\n",
        "print(\"Requested kraken pin:\", KR_PIN)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018844"
      },
      "source": [
        "\n",
        "# 1) Install kraken + deps (with safe fallback to 5.3.0 on Py3.12)\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(*pkgs):\n",
        "    print(\"pip install\", \" \".join(pkgs))\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
        "\n",
        "try:\n",
        "    pip_install(KR_PIN, \"torch>=2.1,<3\", \"cairocffi\", \"opencv-python\", \"lxml\", \"h5py\")\n",
        "except Exception as e:\n",
        "    print(\"Primary install failed:\", e)\n",
        "    KR_PIN = \"kraken==5.3.0\"\n",
        "    pip_install(KR_PIN, \"torch>=2.1,<3\", \"cairocffi\", \"opencv-python\", \"lxml\", \"h5py\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018882"
      },
      "source": [
        "\n",
        "# Verify installs without relying on kraken.__version__\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "import shutil, torch, sys\n",
        "\n",
        "def pkg_ver(name):\n",
        "    try:\n",
        "        return version(name)\n",
        "    except PackageNotFoundError:\n",
        "        return \"not installed\"\n",
        "\n",
        "print(\"python:\", sys.version.split()[0])\n",
        "print(\"kraken:\", pkg_ver(\"kraken\"))\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"ketos on PATH:\", shutil.which(\"ketos\"))\n",
        "!ketos --version || true\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.018915"
      },
      "source": [
        "## Config â€” paths, format, and validation split (Makefile-style)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018944"
      },
      "source": [
        "\n",
        "# ======= CONFIG (EDIT ME) =======\n",
        "# Kraken input format flag for training: one of {\"xml\", \"alto\", \"page\"}\n",
        "FORMAT = \"xml\"\n",
        "\n",
        "# Google Drive roots (add/remove as you like)\n",
        "DRIVE_ROOTS = [\n",
        "    \"/content/drive/MyDrive/avestan_gt\",   # EDIT to your folder(s)\n",
        "    # \"/content/drive/MyDrive/another_dataset\"\n",
        "]\n",
        "\n",
        "# Local roots inside Colab (uploads/extracted zips will live here)\n",
        "LOCAL_ROOTS = [\n",
        "    \"/content/data_local\"\n",
        "]\n",
        "\n",
        "# Validation split fraction\n",
        "VAL_FRACTION = 0.10\n",
        "# ================================\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.018971"
      },
      "source": [
        "### Mount Google Drive (if using DRIVE_ROOTS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.018993"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019016"
      },
      "source": [
        "\n",
        "### Upload local files/zips (optional)\n",
        "Upload **ZIPs** (auto-extracted) or loose files. Everything lands under `/content/data_local`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.01904"
      },
      "source": [
        "\n",
        "import os, io, zipfile\n",
        "from google.colab import files\n",
        "\n",
        "os.makedirs(\"/content/data_local\", exist_ok=True)\n",
        "print(\"Upload zips or files (you can select multiple). Zips are extracted to /content/data_local.\")\n",
        "uploaded = files.upload()\n",
        "for name, data in uploaded.items():\n",
        "    path = f\"/content/data_local/{name}\"\n",
        "    with open(path, \"wb\") as f: f.write(data)\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "            zf.extractall(\"/content/data_local\")\n",
        "        os.remove(path)\n",
        "print(\"Local root ready at /content/data_local\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019067"
      },
      "source": [
        "## Build merged XML list from all roots + sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019091"
      },
      "source": [
        "\n",
        "import os, glob, xml.etree.ElementTree as ET\n",
        "\n",
        "ROOTS = [p for p in (DRIVE_ROOTS + LOCAL_ROOTS) if os.path.exists(p)]\n",
        "assert ROOTS, \"No existing roots found. Check DRIVE_ROOTS/LOCAL_ROOTS paths.\"\n",
        "\n",
        "xmls = []\n",
        "for root in ROOTS:\n",
        "    found = sorted(glob.glob(os.path.join(root, \"**\", \"*.xml\"), recursive=True))\n",
        "    if found:\n",
        "        print(f\"[OK] {len(found):>5} XMLs in {root}\")\n",
        "        xmls.extend(found)\n",
        "    else:\n",
        "        print(f\"[..]    0 XMLs in {root} (skipped)\")\n",
        "print(\"TOTAL XML files:\", len(xmls))\n",
        "assert len(xmls) >= 3, \"Need at least a few XML pages.\"\n",
        "\n",
        "# Sample check: ensure PAGE/ALTO <Page imageFilename='...'> exists on disk\n",
        "missing = []\n",
        "for xp in xmls[:800]:  # sample first 800; remove slice to check all\n",
        "    try:\n",
        "        root = ET.parse(xp).getroot()\n",
        "    except ET.ParseError:\n",
        "        print(\"XML parse error:\", xp); continue\n",
        "    for pe in root.findall(\".//{*}Page\"):\n",
        "        fn = pe.get(\"imageFilename\")\n",
        "        if fn:\n",
        "            img_path = os.path.join(os.path.dirname(xp), fn)\n",
        "            if not os.path.exists(img_path):\n",
        "                missing.append((xp, fn)); break\n",
        "\n",
        "print(\"Missing referenced images:\", len(missing))\n",
        "if missing[:5]:\n",
        "    print(\"First few missing:\", missing[:5])\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019119"
      },
      "source": [
        "## Create explicit train/val lists (like Makefile prereqs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019142"
      },
      "source": [
        "\n",
        "import random, pathlib\n",
        "random.seed(42)\n",
        "\n",
        "random.shuffle(xmls)\n",
        "n_val = max(1, int(len(xmls)*VAL_FRACTION))\n",
        "val_xmls = xmls[:n_val]\n",
        "train_xmls = xmls[n_val:]\n",
        "\n",
        "pathlib.Path(\"/content/lists\").mkdir(parents=True, exist_ok=True)\n",
        "open(\"/content/lists/train.txt\",\"w\").write(\"\\n\".join(train_xmls))\n",
        "open(\"/content/lists/val.txt\",\"w\").write(\"\\n\".join(val_xmls))\n",
        "\n",
        "len(train_xmls), len(val_xmls)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019168"
      },
      "source": [
        "## Hyperparameters (edit once; used by targets below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019203"
      },
      "source": [
        "\n",
        "# ======= EDIT ME (Hyperparameters) =======\n",
        "import torch\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# Recognition\n",
        "REC_EPOCHS = 30\n",
        "REC_BATCH  = 8\n",
        "REC_OPTIM  = \"Adam\"   # \"Adam\" or \"SGD\"\n",
        "REC_LR     = 1e-4     # If your Kraken errors on --lr, use --lrate in the command cell\n",
        "REC_WD     = 1e-5     # weight decay\n",
        "\n",
        "# Segmentation\n",
        "SEG_EPOCHS = 20\n",
        "SEG_BATCH  = 2\n",
        "SEG_OPTIM  = \"Adam\"\n",
        "SEG_LR     = 5e-4\n",
        "# =========================================\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019229"
      },
      "source": [
        "## Target: `train_seg` â€” Segmentation training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019252"
      },
      "source": [
        "\n",
        "# Create output dir\n",
        "!mkdir -p /content/models\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019282"
      },
      "source": [
        "\n",
        "# ===== train_seg =====\n",
        "# To adjust learning rate, modify --lr (or --lrate if your Kraken is older).\n",
        "# To change epochs, modify --epochs.\n",
        "# To change batch-size, modify --batch-size.\n",
        "# To continue training from a prior segmentation model, add:\n",
        "#    --load-model /path/to/seg_base.mlmodel\n",
        "\n",
        "import shlex, os\n",
        "\n",
        "seg_cmd = f'''\n",
        "ketos segtrain -f {shlex.quote(FORMAT)} @/content/lists/train.txt \\\n",
        "  -o /content/models/seg \\\n",
        "  --device {shlex.quote(DEVICE)} \\\n",
        "  --epochs {SEG_EPOCHS} \\\n",
        "  --batch-size {SEG_BATCH} \\\n",
        "  --optimizer {shlex.quote(SEG_OPTIM)} --lr {SEG_LR} \\\n",
        "  --validation @/content/lists/val.txt\n",
        "'''\n",
        "print(seg_cmd)\n",
        "os.system(seg_cmd)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019476"
      },
      "source": [
        "## Target: `train_recog` â€” Recognition training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.01952"
      },
      "source": [
        "\n",
        "# ===== train_recog =====\n",
        "# To adjust learning rate, modify --lr (or --lrate if your Kraken is older).\n",
        "# To change epochs, modify --epochs.\n",
        "# To change batch-size, modify --batch-size.\n",
        "# To continue training from a prior recognizer, add:\n",
        "#    --load-model /path/to/rec_base.mlmodel\n",
        "\n",
        "import shlex, os\n",
        "\n",
        "rec_cmd = f'''\n",
        "ketos train -f {shlex.quote(FORMAT)} @/content/lists/train.txt \\\n",
        "  -o /content/models/rec \\\n",
        "  --device {shlex.quote(DEVICE)} \\\n",
        "  --epochs {REC_EPOCHS} \\\n",
        "  --batch-size {REC_BATCH} \\\n",
        "  --optimizer {shlex.quote(REC_OPTIM)} --lr {REC_LR} --weight-decay {REC_WD} \\\n",
        "  --validation @/content/lists/val.txt\n",
        "'''\n",
        "print(rec_cmd)\n",
        "os.system(rec_cmd)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019553"
      },
      "source": [
        "## Target: `eval` â€” Evaluate recognition (CER/WER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019578"
      },
      "source": [
        "\n",
        "# ===== eval =====\n",
        "# Evaluate the best recognition model on the held-out list:\n",
        "\n",
        "import shlex, os\n",
        "\n",
        "eval_cmd = f'''\n",
        "ketos test -f {shlex.quote(FORMAT)} /content/models/rec_best.mlmodel @/content/lists/val.txt\n",
        "'''\n",
        "print(eval_cmd)\n",
        "os.system(eval_cmd)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019604"
      },
      "source": [
        "## Package models (like `make package`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019628"
      },
      "source": [
        "\n",
        "!cd /content/models && ls -lh && zip -r ../trained_models.zip . && cd /content\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell_1761496179.019653"
      },
      "source": [
        "## Optional: Upload to msia.escriptorium.fr via API (like `make upload`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell_1761496179.019686"
      },
      "source": [
        "\n",
        "# UI upload (My Models â†’ Upload) is simplest.\n",
        "# This cell shows how to POST a model programmatically if your instance supports it.\n",
        "\n",
        "MSIA_URL   = \"https://msia.escriptorium.fr\"\n",
        "API_TOKEN  = \"PASTE_YOUR_TOKEN_HERE\"   # keep secret; or leave blank and use UI upload\n",
        "MODEL_PATH = \"/content/models/rec_best.mlmodel\"   # change to seg_best.mlmodel to upload the segmenter\n",
        "MODEL_NAME = \"rec_best\"\n",
        "\n",
        "if API_TOKEN and API_TOKEN != \"PASTE_YOUR_TOKEN_HERE\":\n",
        "    import requests\n",
        "    headers = {\"Authorization\": f\"Token {API_TOKEN}\"}\n",
        "    files = {\"file\": (MODEL_NAME + \".mlmodel\", open(MODEL_PATH, \"rb\"), \"application/octet-stream\")}\n",
        "    data  = {\"name\": MODEL_NAME}\n",
        "    resp = requests.post(f\"{MSIA_URL}/api/models/\", headers=headers, files=files, data=data)\n",
        "    print(\"Status:\", resp.status_code)\n",
        "    try:\n",
        "        print(resp.json())\n",
        "    except Exception:\n",
        "        print(resp.text[:800])\n",
        "else:\n",
        "    print(\"Skipping API upload. Use UI upload or paste a valid API token.\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ]
}