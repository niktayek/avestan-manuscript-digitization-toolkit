{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078f11f4",
   "metadata": {},
   "source": [
    "\n",
    "# Kraken + ALTO OCR ‚Äî Colab (Per‚ÄëManuscript **Project Folder** on Drive)\n",
    "\n",
    "**One input:** set `PROJECT_ID` (your manuscript ID).  \n",
    "This notebook will create and use a **project folder on Drive** so that **everything** for that manuscript stays together:\n",
    "\n",
    "```\n",
    "MyDrive/kraken_projects/<PROJECT_ID>/\n",
    "‚îú‚îÄ‚îÄ data/           # upload & extracted dataset (ALTO XML + images)\n",
    "‚îî‚îÄ‚îÄ models/\n",
    "    ‚îú‚îÄ‚îÄ rec/        # recognition models (attempt_01.mlmodel, ...)\n",
    "    ‚îî‚îÄ‚îÄ seg/        # segmentation models (optional)\n",
    "```\n",
    "\n",
    "What you get:\n",
    "- ALTO‚Äëfirst pairing, ZIP upload into the **project data/** folder.\n",
    "- **Auto attempt detection**: cross‚Äëmanuscript warm‚Äëstart for attempt_01 if available; resume & lower LR later.\n",
    "- **CPU speed boosts** (threads, Pillow‚ÄëSIMD, tuned batch/workers).\n",
    "- Lean Drive usage (small pip cache only).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d4ab",
   "metadata": {},
   "source": [
    "## 1) Connect Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive  # type: ignore\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Drive mounted at /content/drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daffa535",
   "metadata": {},
   "source": [
    "## 2) Project Settings (only edit `PROJECT_ID`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title üîß Project Settings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_ID = \"0093\"  #@param {type:\"string\"}\n",
    "\n",
    "ROOT_IN_DRIVE = \"/content/drive/MyDrive\"\n",
    "PROJECTS_ROOT = f\"{ROOT_IN_DRIVE}/kraken_projects\"\n",
    "PROJECT_DIR   = f\"{PROJECTS_ROOT}/{PROJECT_ID}\"\n",
    "DATA_DIR      = f\"{PROJECT_DIR}/data\"\n",
    "MODELS_ROOT   = f\"{PROJECT_DIR}/models\"\n",
    "REC_MODELS    = f\"{MODELS_ROOT}/rec\"\n",
    "SEG_MODELS    = f\"{MODELS_ROOT}/seg\"\n",
    "PIP_CACHE_DIR = f\"{ROOT_IN_DRIVE}/.pip-cache\"   # small cache only\n",
    "LISTS_DIR     = f\"{PROJECT_DIR}/lists\"          # keep lists with project\n",
    "\n",
    "# Create the full project tree\n",
    "for p in [PROJECTS_ROOT, PROJECT_DIR, DATA_DIR, MODELS_ROOT, REC_MODELS, SEG_MODELS, PIP_CACHE_DIR, LISTS_DIR]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_LIST = f\"{LISTS_DIR}/train.txt\"\n",
    "VAL_LIST   = f\"{LISTS_DIR}/val.txt\"\n",
    "\n",
    "# Auto‚Äëdetect cores; keep 1 for OS\n",
    "CORES = os.cpu_count() or 2\n",
    "CPU_THREADS = max(2, CORES - 1)\n",
    "DEVICE = \"cpu\"   # set \"cuda\" if you enable a T4 GPU in Colab\n",
    "\n",
    "print(\"‚úÖ Project folders ready\")\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"REC_MODELS:\", REC_MODELS)\n",
    "print(\"SEG_MODELS:\", SEG_MODELS)\n",
    "print(\"LISTS_DIR:\", LISTS_DIR)\n",
    "print(\"CPU_THREADS:\", CPU_THREADS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f2791",
   "metadata": {},
   "source": [
    "## 3) CPU Speed Boost (threads & math libs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(CPU_THREADS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(CPU_THREADS)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(CPU_THREADS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(CPU_THREADS)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"0\"\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "\n",
    "for k in [\"OMP_NUM_THREADS\",\"MKL_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"KMP_BLOCKTIME\",\"KMP_AFFINITY\"]:\n",
    "    print(k, \"=\", os.environ.get(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3616394",
   "metadata": {},
   "source": [
    "## 4) Install Kraken (lean) + Pillow‚ÄëSIMD (faster image IO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, subprocess, shlex\n",
    "\n",
    "os.environ[\"PIP_CACHE_DIR\"] = PIP_CACHE_DIR\n",
    "os.environ[\"PIP_DISABLE_PIP_VERSION_CHECK\"] = \"1\"\n",
    "os.environ[\"PIP_NO_INPUT\"] = \"1\"\n",
    "\n",
    "def is_importable(pkg: str) -> bool:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if is_importable(\"kraken\"):\n",
    "    import kraken\n",
    "    print(f\"‚úÖ Kraken available (version: {getattr(kraken, '__version__', 'unknown')})\")\n",
    "else:\n",
    "    print(\"‚è≥ Installing Kraken (lean) ...\")\n",
    "    subprocess.run(shlex.split(\"python -m pip -q install --upgrade pip\"), check=True)\n",
    "    subprocess.run(shlex.split(\n",
    "        \"python -m pip -q install --prefer-binary --upgrade-strategy only-if-needed \"kraken[cairo]\"\"\n",
    "    ), check=True)\n",
    "    import kraken, importlib\n",
    "    importlib.reload(kraken)\n",
    "    print(f\"‚úÖ Installed Kraken (version: {getattr(kraken, '__version__', 'unknown')})\")\n",
    "\n",
    "# Faster image decoding: Pillow-SIMD\n",
    "print(\"‚è≥ Switching to Pillow-SIMD for faster image ops...\")\n",
    "subprocess.run(shlex.split(\"python -m pip -q uninstall -y pillow\"), check=True)\n",
    "subprocess.run(shlex.split(\n",
    "    \"python -m pip -q install --prefer-binary --upgrade-strategy only-if-needed pillow-simd\"\n",
    "), check=True)\n",
    "print(\"‚úÖ Pillow-SIMD installed\")\n",
    "\n",
    "def maybe_purge_cache(purge: bool = False):\n",
    "    if purge:\n",
    "        print(\"Purging pip cache on Drive...\")\n",
    "        subprocess.run(shlex.split(\"python -m pip cache purge\"), check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3a425",
   "metadata": {},
   "source": [
    "## 5) Upload your ALTO dataset (ZIP ‚Üí Drive project folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files  # type: ignore\n",
    "import zipfile, os\n",
    "\n",
    "print(\"üì¶ Please select your ZIP (ALTO XML + images)...\")\n",
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    raise SystemExit(\"‚ùå No file uploaded.\")\n",
    "\n",
    "zip_name = next(iter(uploaded.keys()))\n",
    "zip_path = f\"/content/{zip_name}\"\n",
    "\n",
    "# Extract into the Drive project data folder\n",
    "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "    zf.extractall(DATA_DIR)\n",
    "\n",
    "print(f\"‚úÖ Extracted into: {DATA_DIR}\")\n",
    "!find \"$DATA_DIR\" -maxdepth 2 -type f | head -n 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4c0f",
   "metadata": {},
   "source": [
    "## 6) Build train/val lists from ALTO (in project folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"}\n",
    "\n",
    "def _strip_ns(tag: str) -> str:\n",
    "    return tag.split('}', 1)[1] if '}' in tag else tag\n",
    "\n",
    "def alto_image_from_xml(xml_path: Path) -> Optional[str]:\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for el in root.iter():\n",
    "            if _strip_ns(el.tag) == \"fileName\":\n",
    "                if el.text and el.text.strip():\n",
    "                    return el.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def find_image_candidates(root: Path) -> dict:\n",
    "    images = {}\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            images.setdefault(p.stem, str(p.resolve()))\n",
    "    return images\n",
    "\n",
    "def resolve_image_for_alto(xml_path: Path, data_root: Path, images_by_stem: dict) -> Optional[str]:\n",
    "    fn = alto_image_from_xml(xml_path)\n",
    "    if fn:\n",
    "        candidate = (xml_path.parent / fn)\n",
    "        if candidate.exists():\n",
    "            return str(candidate.resolve())\n",
    "        for p in data_root.rglob(Path(fn).name):\n",
    "            if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "                return str(p.resolve())\n",
    "    stem = xml_path.stem\n",
    "    return images_by_stem.get(stem)\n",
    "\n",
    "def find_pairs_alto_first(root: str) -> List[Tuple[str, str]]:\n",
    "    rootp = Path(root)\n",
    "    images_by_stem = find_image_candidates(rootp)\n",
    "    pairs: List[Tuple[str, str]] = []\n",
    "    for xml in rootp.rglob(\"*.xml\"):\n",
    "        try:\n",
    "            with open(xml, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                head = fh.read(4096)\n",
    "                if \"<alto\" not in head:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "        img_path = resolve_image_for_alto(xml, rootp, images_by_stem)\n",
    "        if img_path:\n",
    "            pairs.append((img_path, str(xml.resolve())))\n",
    "    return pairs\n",
    "\n",
    "def write_list(pairs: List[Tuple[str, str]], out_path: str):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for img, xml in pairs:\n",
    "            f.write(f\"{img}\\t{xml}\\n\")\n",
    "\n",
    "pairs = sorted(set(find_pairs_alto_first(DATA_DIR)))\n",
    "n = len(pairs)\n",
    "print(f\"Found {n} image+ALTO pairs.\")\n",
    "\n",
    "if n < 2:\n",
    "    raise SystemExit(f\"‚ùå Not enough samples in {DATA_DIR}. Found {n}. Check your ZIP structure.\")\n",
    "\n",
    "# 90/10 split\n",
    "cut = max(1, int(n * 0.9))\n",
    "train_pairs, val_pairs = pairs[:cut], pairs[cut:]\n",
    "write_list(train_pairs, TRAIN_LIST)\n",
    "write_list(val_pairs,   VAL_LIST)\n",
    "print(f\"‚úÖ Wrote lists ‚Üí {TRAIN_LIST} ({len(train_pairs)}), {VAL_LIST} ({len(val_pairs)})\")\n",
    "\n",
    "print(\"\\nSample train lines:\")\n",
    "print(\"\\n\".join(open(TRAIN_LIST, encoding=\"utf-8\").read().splitlines()[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71729d97",
   "metadata": {},
   "source": [
    "## 7) Auto‚Äëdetect attempt and choose base model (within Drive projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44173fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "\n",
    "def list_attempt_models(models_dir: str):\n",
    "    return sorted(Path(models_dir).glob(\"attempt_*.mlmodel\"))\n",
    "\n",
    "def next_attempt_id(models_dir: str) -> int:\n",
    "    attempts = list_attempt_models(models_dir)\n",
    "    if not attempts:\n",
    "        return 1\n",
    "    nums = []\n",
    "    for p in attempts:\n",
    "        m = re.search(r\"attempt_(\\d+)\\.mlmodel$\", p.name)\n",
    "        if m:\n",
    "            nums.append(int(m.group(1)))\n",
    "    return (max(nums) + 1) if nums else 1\n",
    "\n",
    "def find_previous_attempt_model(models_dir: str, attempt_id: int) -> str or None:\n",
    "    prev_id = attempt_id - 1\n",
    "    if prev_id < 1:\n",
    "        return None\n",
    "    cand = Path(models_dir) / f\"attempt_{prev_id:02d}.mlmodel\"\n",
    "    return str(cand) if cand.exists() else None\n",
    "\n",
    "def newest_model_from_other_projects(projects_root: str, exclude_project: str) -> str or None:\n",
    "    # Search all rec models under kraken_projects/*/models/rec/*.mlmodel\n",
    "    pattern = str(Path(projects_root) / \"*\" / \"models\" / \"rec\" / \"*.mlmodel\")\n",
    "    newest = None\n",
    "    newest_mtime = -1\n",
    "    for p in glob.glob(pattern):\n",
    "        if f\"/{exclude_project}/\" in p or f\"\\\\{exclude_project}\\\\\" in p:\n",
    "            continue\n",
    "        try:\n",
    "            mtime = os.path.getmtime(p)\n",
    "            if mtime > newest_mtime:\n",
    "                newest_mtime = mtime\n",
    "                newest = p\n",
    "        except Exception:\n",
    "            pass\n",
    "    return newest\n",
    "\n",
    "ATTEMPT_ID = next_attempt_id(REC_MODELS)\n",
    "OUT_MODEL = str(Path(REC_MODELS) / f\"attempt_{ATTEMPT_ID:02d}.mlmodel\")\n",
    "\n",
    "if ATTEMPT_ID == 1:\n",
    "    BASE_MODEL = newest_model_from_other_projects(PROJECTS_ROOT, PROJECT_ID)\n",
    "    if BASE_MODEL:\n",
    "        print(f\"‚ÑπÔ∏è Attempt {ATTEMPT_ID:02d}: using cross‚Äëmanuscript base ‚Üí {BASE_MODEL}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Attempt {ATTEMPT_ID:02d}: starting from scratch.\")\n",
    "else:\n",
    "    BASE_MODEL = find_previous_attempt_model(REC_MODELS, ATTEMPT_ID)\n",
    "    if BASE_MODEL:\n",
    "        print(f\"‚ÑπÔ∏è Attempt {ATTEMPT_ID:02d}: resuming from previous attempt ‚Üí {BASE_MODEL}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Attempt {ATTEMPT_ID:02d}: previous attempt not found; starting from scratch.\")\n",
    "\n",
    "LR_FOR_LATER = 1e-4\n",
    "AUTO_LR = LR_FOR_LATER if ATTEMPT_ID > 1 else None\n",
    "\n",
    "# CPU batch heuristic\n",
    "BATCH_SIZE = min(32, max(8, (os.cpu_count() or 2) * 2))\n",
    "\n",
    "print(f\"ATTEMPT_ID: {ATTEMPT_ID:02d}\\nOUT_MODEL: {OUT_MODEL}\\nBASE_MODEL: {BASE_MODEL}\\nAUTO_LR: {AUTO_LR}\\nBATCH_SIZE: {BATCH_SIZE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d485914",
   "metadata": {},
   "source": [
    "## 8) Train recognition model (in project folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d23315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"ketos\",\"train\",\n",
    "    \"-o\", OUT_MODEL,\n",
    "    \"--workers\", str(int(CPU_THREADS)),\n",
    "    \"--device\", DEVICE,\n",
    "    \"--batch-size\", str(int(BATCH_SIZE)),\n",
    "    \"-f\", \"alto\",\n",
    "    TRAIN_LIST, VAL_LIST\n",
    "]\n",
    "if BASE_MODEL:\n",
    "    cmd += [\"--load\", BASE_MODEL]\n",
    "if AUTO_LR is not None:\n",
    "    cmd += [\"--lr\", str(AUTO_LR)]\n",
    "\n",
    "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "result = subprocess.run(cmd, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"‚úÖ Training finished. Model at: {OUT_MODEL}\")\n",
    "else:\n",
    "    raise SystemExit(\"‚ùå Training failed. Check logs above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afeea0d",
   "metadata": {},
   "source": [
    "## 9) Evaluate (CER/WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da1232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shlex, subprocess\n",
    "\n",
    "cmd = [\"ketos\", \"test\", \"-f\", \"alto\", \"-m\", OUT_MODEL, VAL_LIST]\n",
    "print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "res = subprocess.run(cmd, text=True)\n",
    "\n",
    "if res.returncode == 0:\n",
    "    print(\"‚úÖ Evaluation completed.\")\n",
    "else:\n",
    "    raise SystemExit(\"‚ùå Evaluation failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22013743",
   "metadata": {},
   "source": [
    "\n",
    "## 10) (Optional) Train a **Segmentation** model (same project)\n",
    "\n",
    "If you have **segmentation ground truth** (e.g., PAGE‚ÄëXML/POLY) you can train a segmentation model and keep it under:\n",
    "```\n",
    "{SEG_MODELS}/attempt_XX.mlmodel\n",
    "```\n",
    "\n",
    "> This cell uses a generic command. Adjust file lists/format options to your segmentation data (e.g., `-f page`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example (adjust to your GT format):\n",
    "# - Prepare SEG_TRAIN_LIST / SEG_VAL_LIST pointing to images + PAGE/POLY files\n",
    "# - Change \"-f page\" to your format if needed\n",
    "\n",
    "from pathlib import Path\n",
    "import shlex, subprocess\n",
    "\n",
    "SEG_TRAIN_LIST = f\"{LISTS_DIR}/seg_train.txt\"\n",
    "SEG_VAL_LIST   = f\"{LISTS_DIR}/seg_val.txt\"\n",
    "SEG_ATTEMPT_ID = 1\n",
    "SEG_OUT_MODEL  = f\"{SEG_MODELS}/attempt_{SEG_ATTEMPT_ID:02d}.mlmodel\"\n",
    "\n",
    "if Path(SEG_TRAIN_LIST).exists() and Path(SEG_VAL_LIST).exists():\n",
    "    cmd = [\n",
    "        \"ketos\",\"segtrain\",\n",
    "        \"-o\", SEG_OUT_MODEL,\n",
    "        \"--workers\", str(int(CPU_THREADS)),\n",
    "        \"--device\", \"cpu\",              # change to \"cuda\" if GPU\n",
    "        \"--batch-size\", \"4\",            # tune for RAM\n",
    "        \"-f\", \"page\",                   # or \"polygonal\", etc.\n",
    "        SEG_TRAIN_LIST, SEG_VAL_LIST\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(shlex.quote(x) for x in cmd))\n",
    "    res = subprocess.run(cmd, text=True)\n",
    "    if res.returncode == 0:\n",
    "        print(f\"‚úÖ Segmentation training finished. Model at: {SEG_OUT_MODEL}\")\n",
    "    else:\n",
    "        print(\"‚ùå Segmentation training failed. Check logs above.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No segmentation lists found. Create:\", SEG_TRAIN_LIST, \"and\", SEG_VAL_LIST, \"to enable this step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e3dd0e",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "- **Everything per manuscript** lives under `MyDrive/kraken_projects/<PROJECT_ID>/`.\n",
    "- For attempt_01, we try to **warm‚Äëstart** from the newest recognition model in **other** projects.\n",
    "- Later attempts automatically **resume** and **lower LR**.\n",
    "- Keep Drive small: only a **pip cache** is shared across projects; models stay inside each project folder.\n",
    "- Enable a **T4 GPU** in Colab and set `DEVICE=\"cuda\"` for much faster training.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
